% !TEX encoding = Mac Central European Roman
\documentclass[a4paper,12pt]{article}

\usepackage{amsmath, amssymb, mathtools, verbatim, bm, xcolor, hyperref}
%\usepackage[polish]{babel}
%\usepackage{polski}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
%\usepackage[macce]{inputenc}
%\usepackage[latin2]{inputenc}
\usepackage{epigraph}
\usepackage{physics}
\usepackage[textsize=tiny]{todonotes}
\definecolor{lightgray}{gray}{0.90}
\newtheorem{theorem}{Theorem}
\usepackage{framed}
\renewenvironment{leftbar}[1][\hsize]
{% 
\def\FrameCommand 
{%

    {\hspace{-3pt}\color{black}\vrule width 3pt}%
    \hspace{0pt}%must no space.
    \fboxsep=\FrameSep\colorbox{lightgray}%
}%
\MakeFramed{\hsize#1\advance\hsize-\width\FrameRestore}%
}
{\endMakeFramed}
\setlength{\FrameSep}{0pt}

%\usepackage{showframe}
\usepackage[left=70pt,
			right=70pt,
            top=50pt,
%            textwidth=345pt,
            marginparsep=0pt,
            marginparwidth=70pt,
%            textheight=692pt,
%            footskip=50pt
            ]
           {geometry}
%\textwidth=16.5 truecm
%\textheight=25 truecm
%\hoffset=-2.5 truecm
%\voffset=-2 truecm

\def\baselinestretch{1.2}

%\pagestyle{empty}

\begin{document}

\title{Non-equilibrium systems and growth of complexity}

\author{Michał Mandrysz \\
Instytut Fizyki, Uniwersytet Jagielloński, \\ul. Łojasiewicza
11, 30-348 Kraków, Polska }

\maketitle

\tableofcontents

\newpage

\section{Introduction}

The inspiration for writing this work and thus, for inquiry of the subject of non-equilibrium statistical mechanics has been the ineffable complexity of the world which slowly begin too become within reach of physics. 
In particular, recent developments in non-equilibrium statistical physics have convinced me that the time is ripe for a review of the vast subject concerning the small and the big, the fluctuation theorems on scales of single DNA strand and MaxEP on the planetary scales.

The investigations here presented start from a historical review of the subject (sections 2,3) which presents sometimes some less known points of view and helps in understanding, not only the chronological development of ideas, but also the various paradoxes and misconceptions (such as the source of irreversibility or the meaning of entropy) that arose on the way of intense research. 

Indeed, the topic of far from equilibrium statistical mechanics developed mostly in the last 30-40 years, therefore a short review of linear response near-equilibrium statistical mechanics of Onsager, Kubo and Prigogine is in place and is presented in section \ref{NearEquilibrium}. This is then followed in section \ref{simple-model} by a crude and simple model illustrating the mechanism of entropy lowering during non-equilibrium conditions, thus allowing for formation of complexity.

In section \ref{IrreversibilityMeasure} I pedagogically present sort of a macroscopic "big-brother" of the fluctuation theorems and its distributions of entropy. Building on introduced there measure of irreversibility we transition naturally to section \ref{FluctuationTheorems} and the topic of fluctuation theorems in one of their deterministic versions, leading to derivation of Crooks fluctuation theorem and Jarzynski inequality.

Some worth innovative applications of the latter to self-replicators and traversals energy landscapes are then reviewed in section \ref{CrooksApplications}. In fact the topic is so rich and developing so fast that one could focus on it exclusively. 

In the final section \ref{UnifyingPrinciple} a review of a attention-catching, but incomplete topic of extrema principles for non-equilibrium physics is performed. Subsequently, the most promising result, extending on MaxEnt approach is presented with an example application to planetary climates.

Finally, the topic of quantum mechanics and von Neumann entropy is only briefly scratched in section 3, leaving place for further work.

%TODO: Dodaj wstęp podobny do Umberto Marini Bettolo Marconi
%Fluctuation-Dissipation: Response Theory in Statistical Physics

\newpage
\epigraph{True logic of this world lies in the calculus of probabilities}{\textit{James Clerk Maxwell}}
\section{Historical outline}
\subsection{The founding fathers of thermodynamics}
The history of thermodynamics reaches back to the 1600s when first rudimentary thermoscopes (the ancestor of the thermometer) started to be constructed and a scientists, like Francis Bacon began to formulate the right ideas about the nature of heat. 

It took however until 1850s, after the experiments of James Joule, for the wide scientific community to finally accept heat as a form of energy. The relation between heat and energy was important for the development of steam engines and led to the description of idealized heat engines and their theoretical efficiency in 1824 by Sadi Carnot. 

After that, around 1850 Rudolf Clausius and William Thomson (Lord Kelvin) stated both the First Law (the conservation of total energy) as well as the Second Law (heat does not spontaneously flow from colder to hotter objects). Other formulations followed quickly and the general implications of the laws were understood. 

More important developments came after the recognition by Rudolf Clausius and James Clerk Maxwell in 1850s (first noticed by Daniel Bernoulli in 1738) that gases consist of molecules at motion. This simple idea allowed Maxwell to derive and calculate many macroscopic properties of gases at equilibrium. 

Shortly after that, Rudolf Clausius introduced the notion of entropy, defined as the ratio of heat and temperature and redefined the Second Law stating that for isolated systems this quantity can only increase in time.
%TODO: Describe the problem with defining the temperature

In 1872 Ludwig Boltzmann constructed an equation that he thought could describe the detailed time development of any gas and used it to derive the so-called H-theorem. The theorem stated that a quantity equal to entropy must always increase in time. Therefore it seemed that Boltzmann had successfully proved the Second Law. During his times however, a famous objection was poised known as the Loschmidt paradox which stated basically, that due to time-reversal property of Newton laws the evolution could be run in reverse leading to decrease in entropy. 

The resolution of this paradox was noted much later and should probably classified as hard to grasp or at least hard to get accustomed to, because even today one can find discussions and erroneous statements about the "arrow of time" in the literature. Indeed, those difficulties were noted by Gibbs as well\cite{Gibbs:1928tw}:
“Any method involving the notion of entropy, the very existence of which depends on the second law of thermodynamics, will doubtless seem to many far-fetched, and may repel beginners as obscure and difficult of comprehension.” 
For this reason we will intentionally postpone the discussion to the later part of this paragraph in which we go through it thoroughly and highlight the more recent paths of developments in non-equilibrium thermodynamics and statistical physics.

In responding to some of the other objections Boltzmann realized around 1876 that in a gas there are many more states that seem chaotic and random than seem orderly. This realization led him to argue that entropy must be proportional to the logarithm of the number of possible states of a system and the nature of the Second Law must be probabilistic.

Around 1900, Williard Gibbs formulated statistical mechanics in more general context and introduced the notion of ensemble - a collection of many macroscopically similar copies of the system upon which the notion of ergodicity was built. 
It was argued that if a single particle visits every possible piece of the phase space, then when averaged over a sufficiently long time then a property in question would have the same value if one would instead think of ensembles. 

Gibbs also introduced another definition of entropy, which, as noted by him \cite{Gibbs:1928tw} would only increase in a closed system if it was measured in a "coarse-grained" way in which nearby states were not distinguished. In literature one can sometimes find statements \cite{Evans:2241458} that this property of Gibbs entropy is problematic, but in fact the resolution of this paradox in very similar to the resolution of the Loschmidt paradox.

During the beginning of the XX century, just before 
the development of quantum theory which largely overshadowed the development of thermodynamics, Albert Einstein published his paper on Brownian motion\cite{Einstein:eEYNf903}(on par with Marian Smoluchowski). This was the first work, where the idea of relating the amplitude of the dissipation to that of the fluctuations was employed, which and started a very successful line of investigations, followed by Langevin and the use of stochastic methods.

Then during 1930s Lars Onsager published very influential papers\cite{Onsager:zgWBDrcO, Onsager:sJs1Kffm}, which may be considered as the first out-reach towards non-equilibrium phenomena.
Besides refining the understanding of transport coefficients and their cross relations it hinted on the validity of the so-called Onsager hypothesis or regression hypothesis, a non-equilibrium parallel to the Boltzmann ergodic hypothesis. It states that the relaxation of a macroscopic non-equilibrium perturbation follows the same laws which govern the dynamics of fluctuations in equilibrium systems.

In the 1950s the theory guided by this principle was further extended by  Herbert Callen's and Theodore Welton's fluctuation-dissipation theorem (not presented here) and by Melville Green and Ryogo Kubo in what is now known as Green-Kubo relations\cite{Kubo:1957cl}, a family of surprising relations connecting transport coefficients to the correlation functions of observables - those will be presented in further sections. 

Meanwhile, thanks to the development of numerical methods and early computers the properties of various mechanical models were investigated.
This ultimately led to the discovery of chaos (chaotic behaviour of systems) and development of information theory.

\subsection{The information era and Schrödinger's influence on physics}

In the 1940s Claude Shannon introduced the notion of information quantity \cite{Shannon:429164} and during the 1950s, it was recognized that entropy is simply the negative of Shannon's quantity. This way a fundamental link between information theory and thermodynamics was established. This coincided with the discovery of the structure of DNA by James Watson and Francis Crick and together with written by Erwin Schrödinger, influential book titled "What is life?" sparked enthusiasm and inspired generations of physicists to answer the alluring (though not easy) question of the role of physics in biological processes. 

In any event it probably wouldn't be an exaggeration to say that Schrödinger himself (as he admits), was inspired by the work of German-American physicists Max Delbrück; who helped launch the molecular biology research program in the late 1930s and explained (in main part) the mechanism of heredity and mutation.
Regardless, Schrödinger makes some very essential observations on the nature of living organisms.

First, their operation (living organisms) as a macroscopic system resembles approximately, a purely mechanical system rather than a thermodynamical system. 
Even though their size is far from what is considered a thermodynamic limit, they tend stay unaffected (in special environments) by random molecular motion known as heat and; at the same time, evade the decay towards equilibrium for an unusually long time. This can be seen as, essentially, the definition of a living organisms.

Secondly, he notices that the way an organism accomplishes the above is through the exchange of energy and matter with it's environment, that leaves it's own internal state in low entropy. He withdraws from considerations of free energy, although he acknowledges that the exact physical understanding should be accomplished through it rather than through entropy. Perhaps, worth mentioning is his hypothesis of "life intensity" the term which ought to parallel with the rate at which the system produces entropy or dissipates heat.

Thirdly, each cell depends on very small group of atoms, the genetic code, which determine it's evolution, something unprecedented, beyond the description of ordinary statistical physics. He proposes, that perhaps, a partial explanation for this dynamical behaviour (rather than statistical) can be traced to rigidity and tightness of chemical bonds. However the very vital point Schrödinger tries to make is the hypothesis, that there must exist a yet unknown, new law of physics that would explain fully how order can be produced out of disorder. 

Lastly, even though Schrödinger introduces some quantum mechanics principles, like the uniqueness of Heitler-London bond in order to defend the theory laid down by Delbrück, he assures that quantum indeterminacy should play only marginal role in the future laws of dynamics of living systems\footnote{The possibility that remains is that the origins of life, not their evolution could be quantum mechanical.}.

As mentioned earlier, Schrödinger influence driven many researchers to focus on the topic of non-equilibrium phenomena, however their individual approaches diverged widely, due to, as we will see the resolution of the Loschmidt paradox.

\subsection{The resolution of the Loschmidt paradox }

The Loschmidt paradox confronts the fact that the fundamental equations of motion are time-reversible. How therefore the irreversibility enters the picture?

The answer of statistical physics lies in the time-asymmetric probabilistic way in which we make predictions about the world. 
Besides the pure probabilistic description we need the common sense, axiom of causality in order to obtain the time-asymmetric description.
We use it so frequently implicitly, that we often forget about it \cite{Evans:2241458}. 

Indeed, Boltzmann himself didn't noticed that the way in which he derived the H-Theorem from his equation, implicitly assumed that the particles are uncorrelated before the collisions (through Stosszahlansatz), but become correlated after the collision \cite{Schwabl:2002} \cite{Dorfman:ozm67-zD}, thus causing the time-reversal asymmetry\footnote{This topic is closely related to molecular chaos and the hypothesis of Onsager presented in section \ref{OnsagerRelationsSection}.}.

One can also see this most clearly in a generic example, reviewing the procedure in which we compute some future macroscopic state from an initial state macrostate. The final state is obtained by taking the \textit{sum} of the probabilities over the indistinguishable microstates, on the other hand the initial state is obtained by taking the \textit{avarage} over the initial microstates.

However, if we would consider a scenerio where we either know the initial configuration exactly or are able to study all degrees of freedom at the final state, the arrow of time would indeed disappear as is the case with microscopic or structureless objects\footnote{There is a slight subtlety on the road towards the microscopic description connected with non-monotonic behaviour of entropy which will be discussed later}.

\subsection{Different approaches towards irreversibility and non-equilibrium}

As one might tell from the large amount of literature on the subject \cite{Doyle:wf} \cite{Layzer:1970dx} \cite{Wolfram:552851} \cite{Rovelli:2015tv} \cite{Courbage:1983eo} %TODO: Add more citations
this explanation of irreversibility noticed long time ago by Clausius, Boltzmann\cite{Wolfram:552851}, Kelvin, Maxwell\cite{Anonymous:0uVSJOI5} and also Einstein (in the polemic with Walter Ritz over time-reversal symmetry of Maxwell equations) still leaves dissatisfaction in many.

Different alternatives for explanations of irreversible processes have been proposed over the years, including retarded potentials of electromagnetism, randomness of the radiative process, quantum mechanics, CP violation, fluctuations, cellular automata and even gravity. We now will shortly discuss each of those approaches and their weaknesses, starting our discussion from the most direct one, given Ilya Prigogine.

The motivation for such development was clearly articulated by Prigogine:
"I have always found it difficult to accept this conclusion [macroscopic irreversibility emerging from initial conditions] {...} especially because of the constructive role of irreversible processes. Can dissipative structures be the result of mistakes?" \cite{Prigogine:1978kz}.

His personal dissatisfaction with lack of irreversibility and unitary evolution at the microscopic level led him to postulate a "stochastic" version of microscopic equations of motion. His idea was to change the fundamental laws of physics as to make them irreversible at the microscopic level. The details of this approach, known in literature as Misra-Prigogine-Courbage theory\cite{Courbage:1983eo}, will not be presented here, as in so far no evidence for its validity has been found\cite{Bricmont:7zJsfTpK}.

Somewhat related to it, is the Ritz argument about the irreversibility of Maxwell's laws of electromagnetism. In a polemic with Einstein he states that the retarded and advanced potentials should not be treated on equal footing, to which Einstein disagrees. 
According to John Fox, who was a later commentator of the debate, based on the long lifetimes of fast muons (which are taken as evidence for time dilation) and the speed-of-light gamma rays from rapidly moving sources, the evidence stands in favor of Einstein's explanation\cite{Fox:1965bg}.

Now to see why irreversibility cannot be driven by quantum mechanics one just needs to notice that all quantum phenomena are controlled by Planck's constant, while the manifestations of the irreversibility - such as friction - are clearly macroscopically large. %It's not true completely in case of light...

A different school of thought \cite{KIEFER2005, Barbour:2014hq}, %TODO:[promoted by Roger Penrose] Find Penrose on gravity/time arrow
claims that gravity is the source of irreversibility. However, it is a well known fact that in the absence of gravitational fields friction and irreversibility occurs as well. Beside that, similarly to the quantum case the Newton's constant is to small to have such an effect. It is demonstrable that the magnitude of friction is controlled by atomic physics and electromagnetism and therefore is a \textit{local} effect. 

One of other hypothesised causes of irreversibility is CP violation. 
The weak nuclear interactions violate the CP symmetry which is equivalent to saying that they violate the T symmetry, because to our best knowledge the CPT symmetry is strictly conserved. However the case here, is again similar to the discussion of gravitational and quantum mechanical effects, namely the effect is again to small to explain the friction force. Friction would have to be proportional to the small angle from the Cabibbo-Kobayashi-Matrix matrix.
This is clearly not the case because the friction force is much stronger and it is controlled by electromagnetic collisions - collisions caused by a force whose microscopic description is time-reversal-symmetric.
%TODO: fluctuations, cellular automat
 
To summarise, one can see that the Second Law and irreversibility are intimately connected with statistics, inference methods and our lack of full information about systems.

\subsection{Jaynes formulation of statistical mechanics (MaxEnt)}

In 1957 Edwin Jaynes published an illuminating article on the information theoretical basis of statistical mechanics which, with agreement to our earlier discussion, equated entropy to our lack of knowledge about the system. From this approach it follows that the maximum entropy state i.e. equilibrium state of statistical physics can be viewed through information theory as the least biased state given the available information (e.g. energy constraints).
Statistical mechanics then becomes, in a strict sense, a form of statistical inference rather than a physical theory.

We consider a situation with $n$ potential outcomes and $m<n$ constraints in the form of known values $F_k$ of some functions $f_k$ ($1 \leq k \leq m$).
One then searches for a probability distribution that maximizes Shannon Entropy

\begin{equation}
  S=-\sum_{i=1}^n p_i \log p_i
\end{equation}

subject to the constraints

\begin{equation}
  \langle f_k \rangle \equiv \sum_{i=1}^n p_i f_k(i)=F_k,\ \ \ \  1\leq k \leq m
\end{equation}

This procedure, familiar to obtaining Maxwell-Boltzmann distribution in statistical physics, gives the least-biased probability distribution consistent with the available information.

The practice of MaxEnt approach can also make frequent use of the formula for maximization of relative entropy (negative Kullback-Leibler divergence), which uncovers it's essence,

\begin{equation}
\label{Kullback-Leibler}
    H(p\lor q) = -\sum_i p_i \ln \frac{p_i}{q_i}
\end{equation}


with respect to $p_i$ (new a posteriori distribution). $H(p||q)$ can be interpreted as the information gained by using $p_i$ instead of $q_i$.


After Shannon and Jaynes established the links between information theory, statistical mechanics and thermodynamics, there was a growing need to include the concept of computation as well. Following some preliminary statements by John von Neumann, it was thought that any computational process must necessarily increase entropy.
However in 1970s, Charles Bennett pointed out that it is not the case\cite{Bennett:1973ko}, laying some early groundwork for relating computational and thermodynamic ideas. This interesting topic however will not be developed in this work. Instead we will focus on a more recent line of work driven by a renewed interest toward the theory of fluctuations, considering non-equilibrium systems with reversible Nosé-Hoover thermostats.

In the early 1990s Evans, Cohen and Morriss considered the fluctuations of the entropy production rate in a shearing fluid, and proposed the so called Fluctuation Relation (FR), which represents a general result concerning systems arbitrarily far from equilibrium which, in the near equilibrium,  stays consistent with the Green-Kubo and Onsager relations. 

Starting with the developments of the FR proposed by Evans and Searles\cite{Evans:2002cu} and, in different conditions, by Giovanni Gallavotti and Ezechiel Cohen \cite{Gallavotti:1995gy}, leading to the works of Gavin Crooks\cite{Crooks:2008ta} and Jarzynski\cite{Jarzynski:1997uj}, the last 20 years have produced a whole new theoretical framework which encompasses the previous linear response theory and goes beyond that, including the far from equilibrium phenomena and the behaviour in between micro and macro scale. The last 5 years of this continued research paved the way for Jeremy England and coworkers\cite{England:2015hl, England:2010fb} which began description of objects living in that scale e.g. proteins and DNA molecules, implementing what could be named, Schrödinger's plan. 

\section{Treatments of entropy in the standard contexts}

\subsection{Gibbs entropy}
\label{Gibbs entropy}
The Gibbs Entropy defined with the use of the $N$ particle distribution function $ \rho $ and the Boltzmann constant $k_B$:
\begin{equation}
  S_G = k_B \int \rho \log{\rho},
\end{equation}

and generalizes both results of Boltzmann:
Boltzmann $H$, which is useful only for description of systems of non-interacting molecules\cite{Jaynes:1965gg} and the Boltzmann entropy $S_B = k_B \log{W}$ where $W$ represents the number of possible microscopic configuration of a macrostate (phase volume). 
From this second fact we see that $\log{k_B^{-1} S_G}$ is a measure of the phase volume of microstates or measure of our degree of ignorance as to the true unknown microstate.

Moreover, it can also be demonstrated\cite{Jaynes:1965gg} that the change of Gibbs entropy over a reversible path is equal to Clausius entropy:

\begin{equation}
\begin{aligned}
  \Delta S_G &= \int_1^2 \frac{d\langle K+ B \rangle+ \langle P \rangle d\Omega}{T}
  &= \frac{dQ}{T}.
\end{aligned}
\end{equation}

Here $K$ is the total kinetic energy, $V$ is the interparticle potential and $P$, $T$ are pressure and temperature. 
Due, to the generality of Gibbs entropy we may therefore drop the $G$, and from now on speak of entropy $S$ and it's properties.

In closed Hamiltonian systems Gibbs entropy stays constant.
This feature of entropy was first noted by Gibbs himself and was solved by a coarse-graining procedure\cite{Gibbs:1928tw}. This alleged arbitrariness of this procedure was subject to critique \cite{Evans:2241458}. %TODO: more on coarse-graining

One can propose a thought experiment, leading to paradox and immediately resolving it to see the case more clearly. Consider a case of an simple gas closed in an isolated box container of size $L$ which molecules are localized in an imaginary box-like area of size $L/2$ at time $t_0$ (Figure 1a). %TODO: pics   
It is obvious that the gas will expand, but according to the constancy of Gibbs entropy for an isolated system the entropy will not change. Of course the answer to this apparent paradox is very simple - if we had the ability to wait the time necessary for particles to localize in the volume $L/2$, we would probably not need the Second Law of thermodynamics. In every imaginable case, we  would need to place the particles in the initial state by hand, that is close them in an actual box of size $L/2$ and then release. 
In this scenerio, the final phase space is of course larger than the initial one and Gibbs entropy increases, as expected.

The means of practical use of still non-coarse-grained entropy in closed systems subject to adiabatic change was explained by MaxEnt approach of Jaynes\cite{Jaynes:1965gg}. If we knew that on the beginning, at time $t_0$ the system is in \textit{complete} thermodynamic equilibrium having entropy $S$, then we know that at the later time; after the external adiabatic ceased the new "test" or experimental distribution function will have entropy $S_e \geq S$. By saying that we demand \textit{complete} thermodynamic equilibrium at the beginning, we say that the systems history has to be followed by the experimenter to become confident of the obtained equilibrium, as some otherwise unexplainable exceptions exist, such as the Hahn experiment. %TODO Cite Hahn

It is perhaps worth underlying, that the increase of entropy is linked to our knowledge about the system, rather than anything it is doing internally. This should not come up as particularly surprising as our division between work and heat is somewhat arbitrary.

Moreover, even the exact parameters of entropy depend on the situation, so does their number. We can increase their number as far as we wish and in doing so, we move toward classical deterministic description and the notion of entropy collapses (this is not the case for von Neumann entropy).

\subsection{von Neumann entropy during measurement process}

Although this work is meant to stay within the classical limit, it might be worth while to clear out the notion of entropy in quantum context.

The von Neumann entropy is defined as 

\begin{equation}
  S_{vN}= -\Tr(\hat{\rho} \log{\hat{\rho}}).
\end{equation}

For which the general form of the density matrix operator is

\begin{equation}
	\hat{\rho}=\sum_k p_k \ket{\psi_k}\bra{\psi_k} 
\end{equation}

in case of pure state $\ket{\psi}$ the density matrix is simply
\begin{equation}
  \hat{\rho} = \ket{\psi}\bra{\psi}
\end{equation}

and it is easy to verify that the entropy of a pure state is equal to zero. The entropy of mixed state is always greater than zero.
If the system is in a pure state, it will continue to be in a pure state as long as it stays isolated. For a mixed state, the degree of non-purity measured by the entropy will stay constant as long as it is isolated. This follows from the fact that the time evolution is unitary and the eigenvalues of the density operator therefore do not change with time.

An interesting question one might ask (and not really discussed in textbooks) is how the entropy changes after a measurement of a particle in many-body system which, initially, was in pure state.

Without loss of generalization let's consider an isolated system of two identical particles described solely by their momentum states.
In the scenario of two particles of identical momentum we can write the initial pure state as 
\begin{equation}
  \ket{2,0,0,...}
\end{equation}
which entropy is of course zero.  
In second quantization formalism the measurement of a particle is realized by the field operator $ \hat{\Psi}(x) =\sum_k \phi_k(x)\hat{a}_n$ which annihilates a single particle at position $x$.
Therefore after the measurement of particle at some position $x$, one particle is "virtually" removed from the system under consideration, but the system stays in pure state
\begin{equation}
  \hat{\Psi}(x)\ket{2,0,0,...}=\phi_1(x)\ket{1,0,0,...},
\end{equation}
which entropy is zero. It is important to notice though that our system lost a particle and therefore the systems before and after measurement are not equivalent! Of course, in reality the particle doesn't disappear. After determination of it's position by experiment ($\Delta x \to 0$), the uncertainty of it's momentum approaches infinity ($\Delta p \to \infty$), which means that we can reconstruct the state using a linear combination of states with \textit{any} value of momentum:

\begin{equation}
  c_1\ \ket{2,,0,...}+c_2\ \ket{1,1,0,...}+  c_3\ \ket{1,0,1,...}+...
\end{equation}
where the  squared modulus of the coefficients has to sum up to one ($ \sum_i \left| c_i \right|^2 = 1 $).

Now depending on the precision of the measurement we can recalculate entropy of course getting a value greater than zero. If we would perform the same analysis for a pure state of two particles in different states i.e. $ket{1,1,...}$ then we would obtain an increase of entropy even without accounting for the lost particle.
%TODO Correct also for correlated systems?
This crude example gives a clear illustration of the fact that after \textbf{any} measurement the von Neumann entropy has to increase. However it's change is ultimately related to lost information about the system in the act of the measurement.

There's another interesting feature of quantum entropy, namely inequalities that it fullfills.
If we bipartite the system into subsystems $A$ and $B$ each containing it's own set of commuting observables, then in order to calculate the entropy $S_A$ of a subsystem $A$ we need to calculate the entropy with respect to density matrix traced over the other subsystem, namely
\begin{equation}
  \hat{\rho}^A = \Tr_B \hat{\rho} 
\end{equation}
then in general the following identities are satisfied

\begin{equation}
\begin{aligned}
	S(\rho) &\leq S_A + S_B	\\
	S(\rho) &\geq \left| S_A - S_B \right|
\end{aligned}
\end{equation}

The interpretation of the first inequality is that the full information about the states of the subsystems $A$ and $B$ will in general not be sufficient to give full information about the state of the total system $A+B$. Or in other words, when there are correlations between the two subsystems, these are not seen in the description of $A$ and $B$ separately. 

\section{Near-equilibrium thermodynamics}
\label{NearEquilibrium}
\subsection{Local equilibrium and entropy production}

The term local equilibrium describes the situation in which the thermodynamic quantities of the system such as density, temperature, pressure, etc. can vary spatially and with time, but in each volume element the thermodynamic relations between the values which apply locally there are obeyed. 
Such conditions are possible through the assumption of efficient dissipation of effects imposed by gradients and chemical affinities through molecular collisions. We therefore expect that this to hold for fluids, moderately dense gases and many solids.
Under this assumptions, extension of equilibrium thermodynamics is possible\cite{Anonymous:NJxQY1gt}.

An approached pioneered by Onsager for the entropy for open systems, is an extension of Clausius entropy for isolated systems, which states the variation of entropy $dS$ is the sum of two contributions: 

\begin{equation}
  	dS=dS_i+dS_e,
\end{equation}

where $dS_i$ is the entropy produced within the system (for example by change of microscopic configuration) and $dS_e$ the entropy transferred into or out of the system through it's boundaries.

The Second Law then states that $dS_i$ must be zero for reversible (equilibrium) transformations and positive for non-equilibrium transformations of the system\cite{DeGroot:2013ue}:

\begin{equation}
  dS_i \geq 0,
\end{equation}

or per unit time:

\begin{equation}
  	\frac{dS_i}{dt}=\int \sigma dV \geq 0
\end{equation}

where $\sigma$ is entropy production source per unit volume and $dV$ denotes infinitezimal volume element.

The entropy supplied, $dS_e$ on the other hand may be positive, zero or negative, depending on the interaction of the system with it's surroundings and may be written in terms of entropy flows through the boundaries of the system:

\begin{equation}
  	\frac{dS_e}{dt}=\int J d\Omega \geq 0
\end{equation}

where $J$ denotes the flux and $\Omega$ the boundaries of the system.

For a closed system we have the Clausius relation in terms of exchanged heat:

\begin{equation}
  dS_e=\frac{dQ}{T}.
\end{equation}


The standard formalism of linear irreversible dynamics develops a more explicit expression for entropy production per unit time, assuming that even outside equilibrium (but near) entropy depends only on the same variables as at equilibrium, expanding it as follows: 

\begin{equation}
  \sigma(x,t) = \sum_i J_i(x,t) X_i(x,t) \geq 0.
\end{equation}


The sources of entropy production from the point of view of coarse-grained Gibbs entropy have been intensely studied in non-equilibrium systems. A notable result \cite{Gilbert:1999ff, Goldstein:1998ip} states that entropy production itself is independent of the level of coarse-graining applied to Gibbs entropy.

There are exist however some problems with the assumption of non-negative entropy sources.
For example in an electric circuit close to equilibrium, entropy production is equal to the product of the electric current times the voltage divided by the ambient temperature. If the circuit has a complex impedance, there will necessarily be a phase lag between the applied voltage and the current. Therefore there will exist an interval in which entropy production will be negative\cite{Evans:2241458}. 
This problem is resolved in the context of fluctuation theorems and \textit{dissipation function}.

\subsection{Linear response, regression and fluctuations}

A very common approximation made in the treatment of near-equilibrium thermodynamics is the assumption of linear response. If an adiabatically insulated system is perturbed out of equilibrium (but still very near to it) by some time dependent force $f(t)$, then the response of mean zero observable $\delta X= X-\langle X \rangle_{eq}$  should satisfy the linearity property

\begin{equation}
\label{LiearityProperty}
  \delta X(\lambda f(t),t) = \lambda \delta X(f(t),t)
\end{equation}

Linear response of a system driven from equilibrium can be described in terms of the \textit{time correlation (autocorrelation) function} of the observable $X$ (from now on we will assume that $X$ is mean zero observable, that is $X=\delta X$):
 
\begin{equation}
  C(t)=\langle X(t)X(0) \rangle = \frac{\Tr{X(t)X(0)\rho_{eq}}}{\Tr{\rho_{eq}}}.
\end{equation}
% Change Tr to integrals?
where $\rho_{eq}$ is the equilibrium density function.

With correlation functions, we now study the effect of relaxation towards equilibrium, assuming that the external influence ceased at time $t=0$.
Then a general property of such auto-correlation function for times $t\geq 0$ is called \textit{regression} and follows directly from the Schwarz inequality and $X^2(t)<X^2(0)$ - the assumption of fading disturbance:
\begin{equation}
 | C(t) | \leq C(0)
\end{equation}
In fact in the long time limit we expect to obtain the equilibrium values of observables and
\begin{equation}
  \lim_{t \to \infty} C(t)=0.
\end{equation}

Some further properties useful for further discussion can also be noted.
On the microscopic level of enumerated, time dependent observables $X_i$, the equations of motion are time reversible and time translation invariant\cite{Anonymous:vN0-ttAB}, thus leading to
\footnote{Some of the variables $X_i$ can in fact be odd under time reversal, thus for those $\langle X_i(t+\tau) X_j(t) \rangle = \langle -X_i(t) X_j(t+\tau) \rangle$}:

\begin{equation}
  \langle X_i(t+\tau) X_j(t) \rangle =   \langle X_i(t-\tau) X_j(t) \rangle =  \langle X_i(t) X_j(t+\tau) \rangle
\end{equation}

Then dividing $\tau$ and going with it to the limit $\tau \to 0$ we obtain 

\begin{equation}
\label{CorrelationTimeDerivative}
  \langle \dot{X}_i(t) X_j(t) \rangle = \langle X_i(t) \dot{X}_j(t) \rangle
\end{equation}

Now one might perform an analysis from macroscopic view.
Assume that some system is described by a set of macroscopic variables $\{\bar{X}_i\}$ for $i=1,...,N$ of zero mean $E(\bar{X}_i)=0$, such that a non-zero value of $\bar{X}_i$ corresponds to an average deviation from the equilibrium value due to an applied external force $f$, again we'll assume the case in which the force ceases to exist i.e. $t>0$. 

From experience one then postulates a set of phenomenological coupled equations bringing the system back to equilibrium state: 
\begin{equation}
\label{LinearResponseEq}
  \dot{\bar{X}}_i=-\sum_j \lambda_{ij} \bar{X}_j
\end{equation}

Such coupling between between macroscopic variables is the source of many old relations, such as thermoelectric Peltier and Seeback effects.

The probability of such deviations is then proportional to the phase volume given by exponential of entropy (see \ref{Gibbs entropy}):

\begin{equation}
  P \propto \exp(\frac{S(\bar{X}_1,...,\bar{X}_N)-S_0}{k_B})
\end{equation}

where $S_0$ is the equilibrium value of entropy. Since, we consider near-equilibrium the linear term in the expansion disappears and we're left with

\begin{equation}
  S-S_0 = - \sum_{ij} S_{ij}\bar{X}_i \bar{X}_j
\end{equation}

where $S_{ij}= -\frac{1}{2}\frac{\partial^2{S}}{\partial{\bar{X}_i}\partial{\bar{X}_j}}$ is a positive definite, symmetric matrix. %Check on this later

One then defines so-called \textbf{generalized thermodynamic forces} as

\begin{equation}
  F_i= -\frac{\partial{S}}{\partial{\bar{X}_i}}= \sum_j S_{ij}\bar{X}_j
\end{equation}

From which, by matrix inversion one can obtain again the macroscopic variables $\bar{X}_i$:

\begin{equation}
  \bar{X}_j = \sum_i (S^{-1})_{ji} F_i.
\end{equation}


Inserting those back to equation (\ref{LinearResponseEq}) one gets

\begin{equation}
\label{FluxesOnMacroscopic}
  \dot{\bar{X}}_i=-\sum_j \lambda_{ij} \sum_k (S^{-1})_{jk} F_k = \sum_k \gamma_{ik} F_k.
\end{equation}

\subsection{Onsager relations and hypothesis}
\label{OnsagerRelationsSection}

Lars Onsager\cite{Onsager:zgWBDrcO} shown that $\gamma_{ik}$ from the previous paragraph is in fact symmetric.

We can now do just that by combining equation (\ref{CorrelationTimeDerivative}) with equation (\ref{FluxesOnMacroscopic}), thus obtaining Onsager relations

\begin{equation}
  \gamma_{ij}=\gamma_{ji}.
\end{equation}

In general the relaxation of small macroscopic non-equilibrium disturbances need not to be related to the regression of microscopic fluctuations in the corresponding equilibrium system. 
However, Onsager conjectured that in the linear approximation they should be equal. To see why this is the case we give a heuristic argument for mechanical forces.
If we assume that the external force $f$ couples to the observable $X$ then the Hamiltonian will exhibit an additional\footnote{This comes from small displacements approximation and $f=-\frac{\partial}{\partial{X}} H$.} term $H'=-f X$.
Let's now consider the expression for $\bar{X}$ for time $t < 0$:
%TODO: Justify the approximation a bit better
\begin{equation}
  \bar{X}(0)=\frac{\Tr\{ X(0) e^{-\beta (H-f X)} \}}{\Tr\{ e^{-\beta (H-f X)} \}} \approx \beta f \langle X(0)X(0) \rangle= \beta f C(0)
\end{equation}

where in approximation each exponential was Taylor expanded to first order.
For time $t>0$

\begin{equation}
  \bar{X}(t)=\frac{\langle X(t) e^{-\beta (H-f X)} \rangle}{\langle e^{-\beta (H-f X)} \rangle} \approx \beta f \langle X(t)X(0) \rangle= \beta f C(t)
\end{equation}

Onsager hypothesis can now be seen as simply

\begin{equation}
  \frac{\bar{X}(t)}{\bar{X}(0)}=\frac{C(t)}{C(0)}
\end{equation}

As a practical note on application of Onsager relations, we quote Charles Kittel \cite{Kittel:817295}:

"It is rarely a trivial problem to find the correct choice of (generalized) forces and fluxes applicable to the Onsager relation."


\subsection{Green-Kubo relations}

The Green-Kubo formulae relate the macroscopic, linear transport coefficients of a system to its microscopic equilibrium fluctuations.

A foretaste of the Green-Kubo formalism was already given in the previous section where we considered a small perturbation term $H'=-f X$ to the Hamiltonian $H$. However to keep the presentation simple we will now turn our attention to isothermal case and static force $f$.

The term for small macroscopic deviations of $Y$ due to field $f$ is given by
\begin{equation}
\bar{Y}=\frac{\Tr\{ Y e^{-\beta (H-f X)} \}}{\Tr\{ e^{-\beta (H-f X)} \}} = \Tr\{ Y e^{-\beta (H - F -f X )} \}
\end{equation}
where $F$ denotes the free energy coming from the partition function. This linear response defines the static isothermal susceptibility $\chi_{BA}^T$ by\footnote{Note, that here again $Y$ is assumed to have mean zero}:

\begin{equation}
  \bar{Y} =\chi_{YX}^T f
\end{equation}

One then uses the following identity\cite{Kubo:1957cl}:

\begin{equation}
  e^{\beta (a+b) }=e^{\beta a} (1+\int_0^{\beta} d\lambda\ e^{-\lambda a} b e^{\lambda (a+b)}),
\end{equation}

with $a=H-F$ and $b=-f X$. One notices that the integral part corresponds to the change in density function under which the ensemble average takes part, thus

\begin{equation}
\begin{aligned}
  \bar{Y} &= \int_0^{\beta} d\lambda\ \Tr\{ Y e^{-\lambda (H - F)} X e^{\lambda (H-F-f X)} \} f \approx  \int_0^{\beta} d\lambda\ \Tr\{ Y e^{-\lambda (H - F)} X e^{\lambda (H-F)} \} f \\
  &= \langle Y X \rangle f
\end{aligned}
\end{equation}

where by approximating $f X$ to be small we obtained a special case of Green-Kubo relations defining the cross term susceptibility between observables $X$ and $Y$ in terms of correlation functions in the static force, isothermal case:

\begin{equation}
  \chi_{YX}^T = \langle Y X \rangle.
\end{equation}

As a note, let's mention a famous objection to the Kubo relations poised by van Kampen about the plausibility of taking the linear terms first and then the ensemble average (in general one should do the opposite), however one should remember that the microscopic trajectories of particles affected by the perturbing fields experience a large number of collisions in exceedingly small times ($\approx 10^{-9}s$ for low density gases) which makes the approximation possible\cite{Dorfman:ozm67-zD}.
\subsection{Steady states and the definition of temperature}

The steady state is loosely defined as an emergent state of a system subject to some constant\footnote{Constant in the sense of some finite-time $\tau$ average} driving force $F_e$ or constant fluxes vector $\bm{f}$, for which values of some observables denoted as $A_i$ stabilize after a sufficiently long time, i.e.

\begin{equation}
\begin{aligned}
  \lim_{t \to \infty} \langle A_i(t) \rangle_0 = const\\
  \frac{1}{\tau} \int_0^{\tau} F_e(t) dt =const
\end{aligned}
\end{equation}

where "const" denotes some different (for each $i$) constant values. The ensemble average here is taken with respect to initial probability distribution function.

Sometimes one can impose a condition of vanishing expectation value of $\partial \rho / \partial t$ over the probability density function $p(\rho)$ and non-vanishing expectation value of fluxes vector $\bm{f}$ over the probability density function $p(\bm{f})$\cite{Dewar:2014ek}, i.e.
\begin{equation}
\begin{aligned}
\label{SteadyStates}
  \langle \frac{\partial \rho}{\partial t} \rangle_{p(\bm{\rho})} &= 0 \\
  \langle \bm{f} \rangle_{p(\bm{f})} &\neq 0.
\end{aligned}
\end{equation}

The first definition (constancy of the density function) cannot however strictly hold true, indeed in cases far from equilibrium considered in section \ref{DissipationTheoremSection} it is shown to be false. This case is used as approximation in the MaxEP (described in later sections) approach.

When discussing steady states one often uses Clausius entropy to describe entropy changes related to heat transfer, however the temperature used in it's definition is not always well defined. 

In cases of near-equilibrium that one may call local equilibrium the definition of temperature differences are "smooth" enough, i.e., locally there is a reasonable definition of temperature and the temperature gradient determines the heat flux. 
In the opposite case, it is the molecular kinetics which determines the energy transfer. In most cases it happens much faster and local equilibrium gets established afterwards.

On the other hand, in far from equilibrium conditions there might be several definitions of temperature. One of the solutions provided by Evans et al.\cite{Evans:2241458} is to define temperature of non-equilibrium state by the temperature of the underlying equilibrium state to which the system would otherwise relax. This definition gives consistent results.

\subsection{MinEP}

The most well known contribution of Ilya Prigogine to statistical physics, often called the Minimum Entropy Production (MinEP) principle, sprouts from the analysis of second order excess entropy around a steady state $ (\delta^2 S)_{ss} $. 

If we perturb the system around it's equilibrium state we obtain
\begin{equation}
  S=S_0 + \delta S + \frac{1}{2}\delta^2 S
\end{equation}

This quantity is than used as a Lyapunov function and has benefits over other (not necessarily all) Lyapunov functions one could define. 
Its macroscopic meaning is conserved independently of microscopic details of the system under consideration and is also independent of the nature of particular (possibly inhomogeneous) fluctuations.
It is important however, that this result holds only for steady states near-equilibrium. It is only near-equilibrium that the quantity $ (\delta^2 S)_{ss} $ generates probability of fluctuations, as Prigogine insisted in response to criticism \cite{Nicolis:1979cv}. 

The term "dissipative structures" was also coined by Prigogine.
In Prigogine's view the fluctuations are the trigger for the instabilities (or rather bifurcations in the equations of motion), which in turn give rise to spacetime structures, called poetically "dissipative structures". 

An often given example of instabilities leading to formation of structures are the Rayleigh-Bénard convection cells, which simplified non-equilibrium (not MinEP) treatment we describe in the next section.

\section{Thermodynamic lowering of entropy in non-equilibrium conditions}
\label{simple-model}

The Second Law of course holds for isolated systems as a whole and one can therefore imagine (on the basis of additivity of entropy) that out of equilibrium some subsystems may maintain lower entropy.

Let's consider a simple model consisting of three elements: the cooler $C$, the heater $H$ and the system under consideration $S$, staying out of equilibrium.
We assume, that the temperatures of the cooler and the heater stay constant, and that heat $Q_H$ flows into the system $S$ and heat $Q_C$ flows out. The situation is illustrated by the picture \ref{Fig2}.
\begin{figure}[ht!]
\centering \includegraphics[width=6cm]{system} \caption{System (S) model}
\label{Fig2} 
\end{figure}

Treating the heater and the cooler as the environment, we can think of our system $S$ as an open.
Further on we'll analyze the system $S$ from the perspective of internal $(i)$ entropy production
and external $(e)$ entropy flux, flowing \emph{to} the system $S$. 
Of course the change in entropy will be the sum of those two contributions:
\begin{equation}
dS_S=dS_i+dS_e.
\label{entrosum}
\end{equation}

In the current analysis let's consider a situation in which the same amount of heat flows in as flows out, that is $dQ_C=-dQ_H$. Using this relation we get the following term for the change of entropy:
\begin{equation}
dS_e=\frac{dQ_H}{T_H}+\frac{dQ_C}{T_C}=dQ_H\left(\frac{1}{T_H}-\frac{1}{T_C}\right)
=dQ_H\left(\frac{T_C-T_H}{T_HT_C}\right)<0.
\label{dSe1}
\end{equation}
From which it follows, that the heat flow takes the entropy out of our system.
For the purpose of further discussion we introduce the concept of rate of entropy change connected with the heat flow:
\begin{equation}
j_e \equiv  \frac{dS_e}{dt}. 
\end{equation}
In the considered scenerio, the $j_e$ is held constant (steady-state) and we suspect a continuous fall in system's entropy.

Yet, moving away from the equilibrium state we suspect, that the a balancing role will be played by $dS_i$ moving the system back to equilibrium state. Similarly, as before we define the rate of internal entropy production:

\begin{equation}
j_i \equiv \frac{dS_i}{dt}.   
\end{equation} 

When $T_H=T_C$, i.e. the system is in equilibrium with constant entropy $S_{EQ}$ then it follows that $j_i=0$.
Therefore the rate of internal entropy production $j_i$ should be a function  of system's entropy $S_S$, i.e. $j_i = j_i(S_S)$ with the boundary condition $j_i(S_S=S_{EQ})=0$. 

Near the equilibrium state $S_S=S_{EQ}$, we can Taylor expand the function $j_i(S_S)$ to it's linear term
\begin{equation}
j_i(S_S)=j_i\left(S_{EQ}\right)+\left(S_S-S_{EQ}\right)C_1+\mathcal{O}\left(S_S^2\right),
\end{equation} 
fulfilling $j_i\left(S_{EQ}\right)=0$. 

The dimensional and stability analysis tells us that $C_1$ has the dimension of inverse time and in the case of 
$j_e=0$ should simply be equal to $S_{EQ}$, therefore we set $C_1 = -\frac{1}{\tau}$, where $\tau$ is a positive defined relaxation constant.

Using the equation (\ref{entrosum}) we get
\begin{equation}
\frac{dS_S}{dt}=j_i\left(S_S\right)=\left(S_S-S_{EQ}\right)C_1, 
\label{stab}
\end{equation} 

The solution of the equation (\ref{stab}) is then
\begin{equation}
S_S(t) =S_{EQ}+(S_0-S_{EQ})e^{-t/\tau}, 
\end{equation}
where the initial condition was set $S_S(0)=S_0$.


Now we include the term $j_e$ into our considerations.
In this case the equation (\ref{entrosum}) results in the following 
\begin{equation}
\frac{dS_S}{dt}=j_e + j_i\left(S_S\right)=j_e +\frac{S_{EQ}-S_S}{\tau}.
\label{dSSdt}
\end{equation} 

 
Given a boundary condition $S_S(0) =S_{EQ}$ it has a solution
\begin{equation}
S_S(t)=S_{EQ}+j_e\tau \left(1-e^{-t/\tau }\right),
\end{equation} 
where $j_e$ is a negative constant (graph of this function is presented on \ref{Fig4}). 
In the limit $t\rightarrow \infty$ the entropy of the system falls to the minimal value
\begin{equation}
S_{min}=S(t\rightarrow \infty) =S_{EQ}+j_e \tau < S_{EQ}.
\end{equation}

\begin{figure}[ht!]
\centering \includegraphics[width=12cm]{wykres3} 
\caption{Inducing lower entropy with heat flow.}
\label{Fig4} 
\end{figure}

This is of course consistent with the second law of thermodynamics as we are describing an open system.
It is easy to notice that the total entropy change is equal to $dS=dS_i \geq 0$ (for simplicity it was assumed that the heater and cooler don't act as producers of entropy).

We see that if the relaxation constant $\tau = 0$ then the system would stay in equilibrium the whole time, of course, $\tau > 0$ for most materials (if not all). The most important part due to which this low entropy state was obtained is of course the entropy out-flow $j_e$ without the non-equilibrium condition would not form.

\section{Measure of irreversibility and the Second Law}
\label{IrreversibilityMeasure}
In the following section we present a general protoplast of the Fluctuation Theorem. It is derived using nothing else than simple probability calculus and the hypothesis of equal apriori probabilities.
In fact, one can obtain from it the exact form of the entropy produced in terms of microscopic transition probabilities for macroscopic objects.

Let's consider a generic system statistical mechanical system a two times, an initial time $t_0$ and a final time $t_1$, each described by a complete set of possible macrostates $\{A_i\}$ for $ i=1,...,N_A $ and $\{B_j\}$ for $ j=1,...,N_B $  respectively. Each macrostate consists of some number of corresponding microstates denoted by $M_i$ for the initial macrostates and $N_j$ for the final macrostates. The deterministic, microscopic equations of motion then evolve a certain number of the microstates $K_{ij}$ from an initial macrostate $A_i$ to some final macrostate $B_j$.

The probability of the forward transition $P(B_j|A_i)$ is then equal to
\begin{equation}
  P(B_j|A_i)= \frac{K_{ij}}{M_i}
\end{equation}

Now for the time reversed case, that is to obtain $P(A_i|B_j)$, we use the Bayes theorem

\begin{equation}
  P(A_i|B_j)=\frac{P(B_j|A_i)P(A_i)}{P(B_j)},
\end{equation}

but on the way of doing so, we note that $A_i$ is still our "hypothesis" and $B_j$ is our evidence. Now, since we have no a priori knowledge about the initial macrostates, each of them is equally probable $P(A_i)= 1/N_A$.

$P(B_j)$ is then the marginal probability of evidence in all contradicting hypotheses or in other words a normalization factor obtained using the relation 

\begin{equation}
  \sum_i P(A_i|B_j) = 1
\end{equation}
which leads to $P(B_j) = \sum_i P(B_j|A_i)P(A_i)$. Using this we obtain the following expression for the post-diction

\begin{equation}
\begin{aligned}
  P(A_i|B_j) &= \frac{K_{ij}}{M_i} P(A_i) \left( \sum_k \frac{K_{kj}}{M_k}P(A_k) \right)^{-1}\\
  &= \frac{K_{ij}}{M_i} \left( \sum_{k,m} \frac{K_{kj}}{K_{km}} \right)^{-1}
\end{aligned}
\end{equation}

where in the last equation we made use of the fact that $\sum_m K_{km}=M_k $.
This form is especially useful, because the matrix elements can be normalized and effectively we obtain a stochasticly-statistical description.

It is important to emphasize that the conditional probabilities $P(B_j|A_i)$ and $P(A_i|B_j)$ are entirely different in nature - the first represents a prediction, but the second is a post-diction. There is no symmetry between assumptions and assertions in conditional probability calculus.

Now, comparing the probability of forward macroscopic evolution to backward evolution probability one obtains: 

\begin{equation}
\label{MacrostatesPRatio}
  \frac{P(B_j|A_i)}{P(A_i|B_j)}= e^{\ln{\sum_{k,m} \frac{K_{kj}}{K_{km}}}}.
\end{equation}

%TODO: Show that it's positive for all cases!

The interpretation of the term in the exponent, can be done by noticing that there's only one known macroscopic quantity\footnote{Up to a numerical factor - $k_B^{-1}$} that is strictly positive and can change signs after reversing the left side, namely the standard measure of irreversibility - entropy\footnote{Later we will see that in the case of fluctuation theorems this quantity can be also negative. Then our association to Shannon entropy breaks down and in this case a secondary notion of \textit{dissipation} is sometimes introduced.}.

We have thus, described entropy produced by a macroscopic system solely in terms of microscopic transition probabilities between two macroscopic states, without any assumptions about the dynamics and external forces influencing the system. Also this is perhaps the most straight-forward argument against Loschmidt.

With the use real, positive random matrices, satisfying the conditions $\sum_j K_{kj} = 1$ for any $k$, to obtain the distributions for the possible values of entropy produced in the transition $ S = \ln{\sum_{k,m} \frac{K_{kj}}{K_{km}}} $. The results, for $N=10000$ random matrices are presented on figure \ref{Fig4}.

\begin{figure}[ht!]
\centering \includegraphics[width=8cm]{dissipation} \caption{Distribution of entropy produced for $N_A = 2$ and $N_B=2$}
\label{Fig4} 
\end{figure}

A remarkable feature of this results is that the obtained distributions are not gaussian and that most likely value is proportional to the number of states. 
%TODO: More pictures
%TODO: Check how. Maybe $\sqrt{N_A N_B} $?

Despite its generality one has to remember that this result holds for macroscopic systems with multiplicity of macrostates. Have we had just one initial or just on final macrostate the analysis would collapsed to a time-reversable case. As we will see shortly, this is exactly the case covered by the Fluctuation Theorems.

\section{Fluctuation theorems}
\label{FluctuationTheorems}
In the previous section we have seen how irreversibility arrises in macroscopic systems, now we wish to extend it to small systems living on the boundary between micro- and macroscopic descriptions. The crucial element of this extension is the same, natural measure of irreversibility introduced before, however now we will be taking into the account the possible microscopic trajectories of the systems. 
An early indicator that this line of attack might indeed be needed was given in case of the mentioned in \ref{Gibbs entropy}, Hahn spin echoes experiment.

The fluctuation theorems and their derivatives including Jarzynski equality have been demonstrated for a wide range of systems and in a number of experiments including DNA stretching \cite{Collin:2005fx}, optically trapped colloids \cite{Carberry:2007be}, shearing systems \cite{Evans:1993bl}, pendulums \cite{Ciliberto:2010jg}, molecular motors \cite{Seifert:2005it} and various quantum mechanical systems \cite{Monnai:2005ke}.
%TODO: Extract references from evans_chapter_rev.pdf

Historically, the first fluctuation theorem was developed in 1995 by Gallavotti and Cohen\cite{Gallavotti:1995gy}, since then many the conditions of applicability were heavily researched and in result there exist many approaches for deriving fluctuation theorems. 
Those can be roughly described as deterministic \cite{Evans:2002gg}\cite{Evans:2241458} or stochastic \cite{Kurchan:1998}.

In the deterministic framework of Evans, irreversibility finds its origins in non-linear terms which provide a contraction of phase space, in contrast to the more direct irreversibility of the equations of motion found in stochastic descriptions which also has the merit of fewer technical difficulties \cite{Crooks:2008ta}. 

Those technical difficulties have their source in ergodicity. If the aim of ergodic theory is to understand how randomness arises from deterministic constituents, once stochasticity is added "by hand" the question is artificially bypassed.
In fact the Gallavotti-Cohen theorem, which is a stationary fluctuation theorem for systems in contact with a deterministic Gaussian thermostat, breaks down in some systems and in most cases when the forcing is very strong. The Fluctuation Relation in the deterministic case holds only if the system has certain ‘ergodic’ properties\cite{Kurchan:2009ub}. 
However, the assumption that stochastic processes retains it's stochastic character at arbitrary small temporal and spatial scales leads to a conclusion that the rate of production of entropy is infinite at the limit of infinite resolution \cite{Dorfman:ozm67-zD}.

Despite this difficulties the deterministic approach of Searles and Evans will be described for reasons of generality, aesthetics and also because it distances itself from the notion of entropy.

\subsection{The deterministic approach}

The main objective of this section will be the derivation of the dissipation function and Evans-Searles fluctuation relation (theorem) while introducing the bare minimum amount of necessary concepts. On basis of this results the Crooks fluctuation theorem and Jarzyński equality will then be derived.  

In the following considerations we will assume that classical mechanics gives an adequate description of the dynamics. We will also assume that quantum and relativistic effects can be safely ignored.

\subsubsection{Thermostated, but time reversible systems}

The first time-reversible, deterministic thermostats and ergostats (homogeneous thermostats) were invented in the early 1980s by Hoover, Ladd and Moran\cite{Hoover:1982dp}. Prior to this development there was no satisfactory mathematical way of modelling thermostatted non-equilibrium steady states.

The construction of thermostated time reversible systems is usually done inserting some time-reversible, but non-Hamiltonian terms into part of the equations of motion defined as the surroundings. Surroundings in first approximation are assumed to stay far from the system of interest and should not affect the system under consideration.
The work done on a system, should be on average, converted into heat, which is conducted through the system of interest and eventually removed by the non-Hamiltonian terms residing in the remote boundaries.

For Gaussian isokinetic thermostat the mentioned time-reversible term is usually of the form $- S_i \alpha p_i $ where $\alpha$ is determined by the thermostatting condition\footnote{By such construction, the system will relax to an equilibrium state $f_{eq}(\bm{\Gamma})=Z^{-1} e^{-\beta H}\delta(\bm{p}\cdot \bm{S}\cdot \bm{p}-2m K$ where $K$ denotes the kinetic energy of the system and $Z$ the partition function.} and $\textbf{S}$ is a diagonal matrix. The term serves as a mean by which we can add or remove heat from the particles in the reservoir region ($S_i =1$ for reservoir region and $S_i = 0$ outside this region)\footnote{In the case of Nosé Hoover thermostat, $\alpha$ becomes an extra degree of freedom\cite{Evans:2241458}.}.

In case of isolated hyperbolic systems interacting with a thermostat the phase space is usually contracted to an attractor (the sum of Lyapunov exponents is less then zero).
This stays in contrast with time-reversibility of the thermostatted equations of motion. The solution to this apparent paradox is related to the fractal nature of the attractor.
The attractor can be described by a smooth measure in the unstable directions and a fractal measure in the stable directions.
This kind of measures are known as SRB (Sinai-Ruelle-Bowen) measures\cite{Dorfman:ozm67-zD}. 

\subsubsection{Time reversible setup}
Let's consider a closed and adiabatic Hamiltonian system of interacting particles which can exchange energy with its environment in the form of work. For example, it might be desirable to change the mean internal energy, $U=\langle H \rangle$ of the system, by externally manipulating some parameter $\lambda(s)$ (in general dependent on time $s$) in the potential energy function. 
When an (different) external agent does work on a system without changing the underlying equilibrium state of mean energy $U$, we refer to that field as a purely \textit{dissipative field}, denoting it generally by $\bm{F}_e$.

For an externally driven adiabatic system, the rate of increase of $H$ must be identically equal to the rate of work $\dot{W}$ done on the system by the environment, thus

\begin{equation}
\label{AdiabaticWork}
  \dot{W} = \dot{H}^{ad} = \dot{\lambda} \frac{\partial{H}}{\partial{\lambda}} +\dot{\bm{q}}\frac{\partial{H}}{\partial{\bm{q}}} +\dot{\bm{p}}\frac{\partial{H}}{\partial{\bm{p}}}
\end{equation}

where the superscript \textit{ad} emphasises adiabatic conditions. 

In the microscopic picture, the systems phase space $\{\bm{q_1},...,\bm{q_N},\bm{p_1},...,\bm{p_N} \} \equiv (\bm{q},\bm{p})\equiv \bm{\Gamma} $ (where $\bm{q_i}, \bm{p_i} $ are denoting position and conjugate momenta of the particle $i$) evolves according to Hamiltonian equations of motion with additional terms connected with the dissipative field $F_e$ and the thermostat:

\begin{equation}
\begin{aligned}
\label{ThermostattedEq}
  \dot{\bm{q}} &=\frac{\partial H(\bm{\Gamma},s)}{\partial{\bm{p}}}+\bm{C}(\bm{F})\cdot\bm{F}_e(s) \\
  \dot{\bm{p}} &=- \frac{\partial H(\bm{\Gamma},s)}{\partial{\bm{q}}}+\bm{D}(\bm{F})\cdot\bm{F}_e(s) - \alpha(\bm{\Gamma})\bm{S}\cdot\bm{p}
\end{aligned}
\end{equation}

Note that the thermostatting term ($- \alpha(\bm{\Gamma})\bm{S}\cdot\bm{p}$) was added to our otherwise adiabatic system in an ad-hoc manner and the adiabatic work relation of equation (\ref{AdiabaticWork}) reduces to

\begin{equation}
  \dot{W}(\bm{\Gamma},s) = \dot{\lambda} \frac{\partial{H(\bm{\Gamma}},s)}{\partial{\lambda}} - V \bm{J}(\bm{\Gamma})\cdot \bm{F}_e(s)
\end{equation}

where $V$ is the volume of the system, and $\bm{J}(\bm{\Gamma})$ is the dissipative flux due to field $\bm{F}_e(\lambda)$, which one (ignoring the thermostat term) can easily obtain to be:

\begin{equation}
  V \bm{J}(\bm{\Gamma}) = -\Big(\frac{\partial{H}}{\partial{\bm{q}}}\cdot \bm{C}+\frac{\partial{H}}{\partial{\bm{p}}}\cdot \bm{D}\Big).
\end{equation}

Of course, the First Law of Thermodynamics has to be conserved, so that the rate of heat exchange with the thermostat follows from $\dot{Q}=\dot{H}-\dot{W}$ and is given by

\begin{equation}
  \dot{Q}(\bm{\Gamma},s)= -\alpha(\bm{\Gamma})\frac{\partial{H}}{\partial{\bm{p}}}\cdot \bm{S}\cdot \bm{p}.
\end{equation}

The total work done and heat added to the system then depends only on the initial phase space point $\bm{\Gamma}_0$ and time duration $t$. That is, $W(\bm{\Gamma}_0,t)=\int_0^t \dot{W} dt$ and $Q(\bm{\Gamma}_0,t)=-\int_0^t \dot{Q} ds$.

We now wish to define a time reversal mapping $M^T$ (written for short as superscript star) to be an operator acting on phase space (the brackets indicate that the operator acts here on the phase exclusively) 

\begin{equation}
  \bm{\Gamma}^*=M^T[\bm{\Gamma}] \equiv (\bm{q},-\bm{p})
\end{equation}

and a p-Liouvillean operator $iL$ defined by the solution of differential equation 

\begin{equation}
 \dot{\bm{\Gamma}} \equiv iL(\bm{\Gamma})\bm{\Gamma}
\end{equation}

which is given by

\begin{equation}
  \bm{\Gamma}_t = S^t \bm{\Gamma} \equiv \exp[iL(\bm{\Gamma})t]\bm{\Gamma}.
\end{equation}

where the $\bm{\Gamma}_t$ denotes phase evolved to time $t$ nad the $ exp[iL(\bm{\Gamma})t] $ is known as the p-propagator or the phase space propagator.
An easy to check property useful in later part of this work is

\begin{equation}
\label{PhaseTimeDer}
  \frac{d}{dt}(S^t \bm{\Gamma})=iL(\bm{\Gamma})\exp[iL(\bm{\Gamma})t]\bm{\Gamma}=S^t \dot{\bm{\Gamma}}
\end{equation}
%TODO: It doesn't seem perfectly ok...

%left out how M^T acts on iL (page 18)

%\equiv  \dot{\bm{\Gamma}}\cdot \frac{\patial}{\partial{\bm{\Gamma}}}

The time reversal dynamics satisfies an easy to check equation

\begin{equation}
  M^T S^t M^T S^t\ \bm{\Gamma} = \bm{\Gamma},
\end{equation}

where the action of operators $M^T$ and $S^t$ is evaluated from the right side to the left side. It is related to an the existence of time reversed trajectories (anti-trajectories), i.e. if we generate a trajectory starting at $\bm{\Gamma}_0$ and terminating at $\bm{\Gamma}_t$, then under the same dynamics, we start at $\bm{\Gamma}_t^*$ and arrive back to $\bm{\Gamma}_0^*$.
In most cases presented here though, we will be interested in \textit{bundles} of trajectories $d\bm{\Gamma}$ and anti-trajectories $d\bm{\Gamma}^*$ passing through a volume element centered around the point $\bm{\Gamma}$.

\paragraph{Phase space distribution function:}

The phase space distribution function $f(\bm{\Gamma};t)$ gives the probability per unit phase space volume of finding phase members near the phase vector $\bm{\Gamma}$ at time $t$.

\paragraph{Probabilities of phase space trajectories: }
The probability $p(d V_{\bm{\Gamma}_t},t)$, that a phase $\bm{\Gamma}$, will be observed within an infinitesimal phase space volume of size $d V_{\bm{\Gamma}_t}$ about $\bm{\Gamma}_t$ at time $t$, is given by,

\begin{equation}
  p(d V_{\bm{\Gamma}_t},t) = f(\bm{\Gamma}_t;t)d V_{\bm{\Gamma}_t}.
\end{equation}


\paragraph{Ensemble averages:}

Value of any phase function $A(\bm{\Gamma})$ can be obtained with the use of ensemble averages by taking $N_{\bm{\Gamma}} $ time evolved initial phases $\bm{\Gamma}$ consistent with macroscopic constraints

\begin{equation}
  \langle A(t) \rangle = \lim_{N_{\bm{\Gamma}}
 \to \infty} \sum_{j=1}^{N_{\bm{\Gamma}}} A(S^t \bm{\Gamma}_j)/N_{\bm{\Gamma}}
\end{equation}

or in the continuous limit by specifying the initial phase space probability density $f(\bm{\Gamma};0)$ and time-dependent evolution of this density $f(\bm{\Gamma};t)$

\begin{equation}
  \langle A(t) \rangle = \int d\bm{\Gamma} A(\bm{\Gamma}) f(\bm{\Gamma};t) = \int d\bm{\Gamma} A(S^t\bm{\Gamma})f(\bm{\Gamma};0)
\end{equation}

This equation can be seen as an application of equivalence of Heisenberg and Schrödinger representations - either the observable or the state is evolved.

Time stationarity of an ensemble average is then defined simply by
\begin{equation}
\label{StationaryStateDef}
    \langle A(t) \rangle =   \langle A(t+\Delta) \rangle
\end{equation}
for any time $\Delta > 0$.

%TODO: More on ensemble averages can be found on page 35

\paragraph{Ergodicity:}
Stationary system is said to be physically ergodic if the time average of the phase function representing a physical observable, along a trajectory that starts \textit{almost anywhere}\cite{Evans:2241458} in the ostensible phase space, is equal to the ensemble average taken over an ensemble of systems consistent with the small number of macroscopic constraints on the system:

\begin{equation}
    \lim_{t \to \infty} \langle A(t) \rangle = \lim_{t \to \infty} \frac{1}{t} \int_0^t ds\ A(S^s \bm{\Gamma}).
\end{equation}
%TODO: Page 22, another definition on page 74

One may also talk about, so called \textit{ergodic consistency condition} in many context, the reason for this requirement stems from the requirement of existence of distributions sitting in the denominator of various theorems for example in the definition (in later introduced) dissipation function.

\subsubsection{Phase Continuity equation}

%To model a three-dimensional system of $N$ interacting particles, we assume that the microstates were initially distributed according to a given normalized probability distribution function $f(\bm{\Gamma};0)$. As mentioned before we separate our system of $N$ particles into system and surroundings, where the surroundings are assumed to be composed of $N_{th}$ particles. 

%TODO: Construction of the termostat left for later, start at page 24

The motion of the phase space distribution function is governed by a Lagrangian form of the phase continuity equation (also known as \textit{streaming})

\begin{equation}
\label{LagrangianLiouville}
  \frac{df(\bm{\Gamma};t)}{dt}=-f(\bm{\Gamma};t)\frac{\partial}{\partial \bm{\Gamma}} \cdot \dot{\bm{\Gamma}}(\bm{\Gamma}) = -f(\bm{\Gamma};t)\Lambda(\bm{\Gamma})
\end{equation}

this equation follows directly from a well known form of Liouville equation

\begin{equation}
    \frac{\partial f(\bm{\Gamma};t) }{\partial t}
    = -\frac{\partial}{\partial \bm{\Gamma}} \cdot [\dot{\bm{\Gamma}}(\bm{\Gamma}) f(\bm{\Gamma};t)]
     = -(\frac{\partial}{\partial \bm{\Gamma}} \cdot \dot{\bm{\Gamma}} + \dot{\bm{\Gamma}} \cdot \frac{\partial}{\partial \bm{\Gamma}}) f(\bm{\Gamma};t)
\end{equation}

where by moving the last term to the left side we get back equation (\ref{LagrangianLiouville}).

%\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}} 
%\paragraph{}

We say that a system fulfills the \textit{adiabatic incompressibility of phase space} ($AI\bm{\Gamma}$) if in the \textbf{absence} of the thermostatting terms the equations of motion preserve the phase space volume, that is
\begin{equation}
  \Lambda \equiv (\partial / \partial\bm{\Gamma}) \cdot \dot{\bm{\Gamma}}=0
\end{equation}

This condition gives a restriction on coupling tensors $\textbf{C}$ and $\textbf{D}$ of equation (\ref{ThermostattedEq})

\begin{equation}
  \frac{\partial}{\partial{\bm{q}}}\cdot\bm{C}\cdot\bm{F}_e=  \frac{\partial}{\partial{\bm{p}}}\cdot\bm{D}\cdot\bm{F}_e =0
\end{equation}

For thermostatted systems in a driven steady state, a contraction of phase space occurs continually, as the initial phase volume shrinks to a fractal attractor of lower dimension.
It can be shown\cite{Evans:2241458} that for isokinetic or isoenergetic systems with fixed total momentum and satisfying $AI\bm{\Gamma}$, the phase space expansion factor is exactly $\Lambda = - (3N_{th} -4) \bm{\alpha} $, where $N_{th}$ denotes the number of thermostated particles.

Moreover, for appropriate selection of thermostats (including Gaussian isokinetic and Nosé-Hoover thermostat) the phase-space contraction factor is proportional to the rate of heat exchange with the thermostat:

\begin{equation}
  \dot{Q}(\bm{\Gamma})=k_B T \Lambda(\bm{\Gamma}).
\end{equation}

%\noindent\makebox[\linewidth]{\rule{\textwidth}{1pt}} 

If we make the following substitution $\bm{\Gamma} \to S^t\bm{\Gamma}$ in equation (\ref{LagrangianLiouville}) this first-order ordinary differential equation is solved by

\begin{equation}
\label{distributionStreaming}
  f(\bm{\Gamma}_t;t)=\exp[-\int_0^t ds\ \Lambda(\bm{\Gamma}_s)]f(\bm{\Gamma};0).
\end{equation}

The measure of an infinitesimal phase space volume $dV_{\bm{\Gamma}_s}$ centered on the streamed position $S^s\bm{\Gamma} : 0 \leq s \leq t $ along the phase space trajectory also changes, but in the opposite direction (in order to keep the probability constant):

\begin{equation}
\label{PhaseVolumeExpansion}
  dV_{\bm{\Gamma}_s} =\exp[\int_0^t ds\ \Lambda(\bm{\Gamma}_s)]dV_{\bm{\Gamma}_0}.
\end{equation}
 
In a lot of cases the phase space volume goes to zero, while the density approaches infinity\cite{Evans:2241458}.

%TODO: Remark about Hermicity of Liouville page 35

\subsubsection{Dissipation and the macroscopic irreversibility}

The dissipation function serves as mathematical replacement for the entropy production. When entropy production can be defined, it is equal, on average, to the dissipation function. The main advantage though is that, unlike the entropy production, the dissipation function can, for ergodically consistent systems be always well defined \cite{Evans:2241458}.

Another justification for introducing can a new quantity is that in some cases dissipation can be negative, but strictly speaking entropy interpreted in the light of information theory, should be positive. This should not come up as a surprise since we're getting closed to scales at which the notion of thermodynamic entropy emerge. 

%If external fields are applied to the system of particles and the external field does work on the system and if that work can be turned completely into heat that can diffuse out of the system, the external field is termed a \textbf{dissipative field}. If the work can be completely stored in the system in the form of potential energy, the external field is termed non-dissipative.
%
%A simple example of a system that turns from non-dissipative into a dissipative system is sodium chloride. In solid state it's an insulator (internal energy increases through polarization), but when heated to around 1100K, melts and becomes a conductor.

The dissipation function was first properly (not implicitly) defined in 2000 by Searles and Evans\cite{Searles:2000ig}. It is similar to the entropy production, and although it's not a state function it provides description of non-equilibrium systems through various fluctuation theorems.

The most straight forward definition of the dissipation function is derived from the ratio of the probabilities $p$ at time zero, of observing sets of phase space trajectories originating inside infinitesimal volumes of phase space $dV_{\bm{\Gamma}_0}$ and $dV_{\bm{\Gamma}_t^*}\equiv d V(M^T S^t \bm{\Gamma}_0)$:

\begin{equation}
\label{ReversibilityEvans}
  \frac{p(d V_{\bm{\Gamma}_0}, 0)}{p(d V_{\bm{\Gamma}_t^*},0)}= 
  \frac{f(\bm{\Gamma}_0;0)d V_{\bm{\Gamma}}}{f(\bm{\Gamma}_t^*;0)d V_{\bm{\Gamma}_t^*}}
\end{equation}
now by noting that the Jacobian for the time reversal map is unity, $ d V_{\bm{\Gamma}^*}/ d V_{\bm{\Gamma}} =1 $, together with equation (\ref{PhaseVolumeExpansion}) we get


\begin{equation}
  \frac{p(d V_{\bm{\Gamma}_0}, 0)}{p(d V_{\bm{\Gamma}_t^*},0)}=
  \frac{f(\bm{\Gamma}_0;0)}{f(\bm{\Gamma}_t^*;0)} 
  \exp[-\int_0^t ds \ \Lambda(\bm{\Gamma}_s)]
\end{equation}
The logarithm of this equation will now be used as the definition of the time integral of dissipation $\Omega$:

\begin{equation}
  \label{Dissipation}
  \int_0^t ds\ \Omega(\bm{\Gamma}_s)\equiv \ln(\frac{f(\bm{\Gamma}_0;0)}{f(\bm{\Gamma}_t^*;0)}) -\int_0^t ds \ \Lambda(\bm{\Gamma}_s) \equiv \Omega_t(\bm{\Gamma}_0)
\end{equation}


The dissipation function $\Omega_t$ is completely determined for a deterministic trajectory by the initial coordinate, $\bm{\Gamma}_0$, and the duration of the trajectory, $t$. Moreover, the time-reversibility of the dynamics dictates that conjugate pairs of trajectories have the same value of dissipation $\Omega_t$ but with opposite sign:

\begin{equation}
  \Omega_t(\bm{\Gamma}_t^*)=-\Omega_t(\bm{\Gamma}_0).
\end{equation}


One should perhaps underline that this is the place in which we postulate that some time-asymmetry takes place, otherwise the equation \label{ReversibilityEvans} would be equal to one.
A possible interpretation of this equation states that dissipation function is a measure of the temporal asymmetry inherent in sets of trajectories originating from an initial distribution of states.

Usually one defines an auxiliary quantity $\bar{\Omega}_t$ called \textit{time-averaged dissipation} defined through relation $\Omega_t(\bm{\Gamma}) \equiv \bar{\Omega}_t(\bm{\Gamma})t$.

\subsubsection{Evans-Searles Fluctuation Theorem}

If we now choose our initial volume elements $\bm{\Gamma}_0$ in such a way that all the trajectories originating at time zero have the time averaged dissipation function $\bar{\Omega}_t(\bm{\Gamma})=(A \pm \delta A)$, then the probability that time average dissipation takes the value $A$ between $A\pm \delta A$ is given by 

\begin{equation}
  p(\bar{\Omega}_t =A)=\int d\bm{\Gamma}_0\ \delta[\bar{\Omega}_t(\bm{\Gamma}_0)-A]f(\bm{\Gamma}_0,0).
\end{equation}

Similarly, by changing the dummy variable of integration $\bm{\Gamma}_0$, the probability of time average dissipation holding the value $-A$ between $-A\pm \delta A$ is equal to:

\begin{equation}
  p(\bar{\Omega}_t =-A)=\int d\bm{\Gamma}_t^*\ \delta[\bar{\Omega}_t(\bm{\Gamma}_t^*)+A]f(\bm{\Gamma}_t^*,0).
\end{equation}

Combining those two equations with equation (\ref{Dissipation}), we get the Evans-Searles Fluctuation Theorem (ESFT):

\begin{equation}
\label{ESFT}
  \frac{p(\bar{\Omega}_t=A)}{p(\bar{\Omega}_t=-A)}=\exp[A\ t].
\end{equation}
%TODO: Add assumptions - page 52
%TODO: Connection to Kullback-Leibler divergence

This fluctuation relation is valid for arbitrary system size (the thermodynamic limit was not required) and can be applied to small systems observed for short periods of time. The conditions of \textit{ergodic consistency} and microsopic time reversibility are all that is required.
The relation has been verified experimentally by Wang(2002)\cite{Wang:2002hw}, Carberry(2007)\cite{Carberry:2007be}.
%TODO: Add more references - page 53

One should note that this approach considers probabilities of infinitesimal sets of trajectories fulfilling given requirements instead of individual trajectories and only at equilibrium all individual trajectories cancel out.
The underlying cause of irreversibility in this case is not exactly obvious, one can however argue\cite{Evans:2241458} that \textit{causality} is the source of irreversibility, as we have started our considerations with the assumptions of known initial distribution function, which is further under the influence of the field $F_e$ and variation of $\lambda$. If we have instead assumed that the end state is the known state we would get an opposite and the ensemble-averaged dissipation $\langle \Omega_t \rangle$ (presented below) would be negative.

The Second Law of thermodynamics can be derived from ESFT in a trivial manner, by showing that time averages of the ensemble-averaged dissipation are non negative.
\begin{equation}
  \langle\Omega_t\rangle\geq 0,\  \forall t > 0.
\end{equation}

 The proof follows from simple integration of equation (\ref{ESFT}):
\begin{equation}
\begin{aligned}
  \langle \Omega_t \rangle &= \int_{-\infty}^{\infty} dB\ p(\Omega_t=B)B\\
  &=\int_0^{\infty} dB\ p(\Omega_t=B)B +\int_{\infty}^{0} dB\ p(\Omega_t=B)B \\
  &=\int_0^{\infty} dB\ p(\Omega_t=B)B -\int_{0}^{\infty} dB\ p(\Omega_t=-B)B \\
  &= \int_0^{\infty} dB\ p(\Omega_t=B)B(1-\exp[-B]) \geq 0 
\end{aligned}
\end{equation} 

At this point it's useful to define equilibrium system as the system for which, over the phase space domain $D$, the time-integrated dissipation function is identically zero:

\begin{equation}
  \bar{\Omega}_{eq,t}(\bm{\Gamma})=0, \forall\bm{\Gamma}\in  D, \forall t >0 \\
  \Rightarrow \langle \Omega_t \rangle = 0, \forall t > 0.
\end{equation}


Kawasaki identity also known as Non-equilibrium Partition Identity (NPI) was first implied for Hamiltonian systems by Yamada and Kawasaki (1967)\cite{Yamada:1967uo} and is stated as:

\begin{equation}
  \langle \exp[-\bar{\Omega}_t t] \rangle =1.
\end{equation}

This result can also be derived from ESFT given by equation (\ref{ESFT}):

\begin{equation}
\begin{aligned}
  \langle \exp[-\bar{\Omega}_t t] \rangle &= \int_{-\infty}^{\infty} dA\ p(\bar{\Omega}_t=A)\exp[-A t]\\
  &=\int_{-\infty}^{\infty} dA\ p(\bar{\Omega}_t=-A)\\
  &=\int_{-\infty}^{\infty} dA'\ p(\bar{\Omega}_t=A')=1.
\end{aligned}
\end{equation}

%TODO: Practical modification on pages 55-56

\subsubsection{Instantaneous dissipation function}

The dissipation function is a functional of both the dynamical equations that evolve the phase $S^t \bm{\Gamma} = \exp[iL(\bm{\Gamma})t]\bm{\Gamma}$ and also the initial distribution $f(\bm{\Gamma};0)$ and their initial time has to be the same.

One could get an equation independent of this initial time by differentiation of equation (\ref{Dissipation}):

\begin{equation}
\begin{aligned}
  \frac{\partial}{\partial t}\int_0^t ds\ \Omega(\bm{\Gamma}_s) &= \Omega(\bm{\Gamma}_t)\\
  &=\frac{\partial}{\partial t}[\ln{f(\bm{\Gamma};0)}-\ln{f(\bm{\Gamma}_t;0)}
-\int_0^t ds \ \Lambda(\bm{\Gamma}_s)
]\\
&=-\frac{1}{f(\bm{\Gamma}_t;0)}\frac{\partial f(\bm{\Gamma}_t;0)}{\partial t}-\Lambda( \bm{\Gamma}_t)\\
&=-\frac{1}{f(\bm{\Gamma}_t;0)}\frac{\partial(  \bm{\Gamma}_t)}{\partial t}\frac{\partial f(\bm{\Gamma}_t;0)}{\partial(\bm{\Gamma}_t)}-\Lambda(\bm{\Gamma}_t)\\
&=-\frac{1}{f(\bm{\Gamma}_t;0)}S^t\dot{\bm{\Gamma}}
\frac{\partial f(\bm{\Gamma}_t;0)}{\partial(S^t \bm{\Gamma})}-\Lambda( \bm{\Gamma}_t)
\end{aligned}
\end{equation}

where the last line was obtained using equation (\ref{PhaseTimeDer}). 
If we now set $t=0$ we obtain the expression for the \textit{instantaneous dissipation function}:

\begin{equation}
  \Omega(\bm{\Gamma})=-\frac{1}{f(\bm{\Gamma};0)}\dot{\bm{\Gamma}}(\bm{\Gamma})\frac{\partial{f(\bm{\Gamma};0)}}{\partial{\bm{\Gamma}}}-\Lambda(\bm{\Gamma})
\end{equation}

\subsubsection{Dissipation Theorem}
\label{DissipationTheoremSection}
As we have seen the dissipation function takes a central role in the fluctuation theorem and the second law inequality. However dissipation is also important in quantifying a range of non-equilibrium behaviours, including nonlinear response and relaxation towards equilibrium.
Starting from the solution of the Lagrangian form of the Liouville equation (\ref{distributionStreaming}) we can use dissipation function equation (\ref{Dissipation}) to derive

\begin{equation}
\begin{aligned}
  f(S^t\bm{\Gamma};t) &= \exp[-\int_0^t ds\ \Lambda(S^s\bm{\Gamma})]f(\bm{\Gamma};0)\\
  &=\exp[-\int_0^t ds\ \Lambda(S^s\bm{\Gamma})]f(S^t \bm{\Gamma};0) \exp[\int_0^t ds\ \Omega(S^s \bm{\Gamma}) + \int_0^t ds\ \Lambda(S^s\bm{\Gamma})]\\
  &=f(S^t \bm{\Gamma};0) \exp[\int_0^t ds\ \bm{\Omega}(S^s \bm{\Gamma})],
\end{aligned}
\end{equation}

after substitution $\bm{\Gamma} \to S^{-t}\bm{\Gamma}$ and change of variables we get 

\begin{equation}
\label{distributionPropagator}
    f(\bm{\Gamma};t)=f(\bm{\Gamma};0)\exp[\int_0^t ds\ \bm{\Omega}(S^{-s} \bm{\Gamma})],
\end{equation}

which states that the forward in time propagator for the N-particle distribution function is given by the exponential (backward) time integral of the dissipative function. 
An immediate conclusion one can draw from this is that for all non-equilibrium deterministic systems the N-particle distribution function has explicit time dependence and cannot be written in a closed, time-stationary form.
As with ESFT, this result can be applied to any initial ensemble and time-reversible dynamics satisfying $AI\bm{\Gamma}$, more detailed analysis of the time-dependant case can be found in \cite{Williams:2008ft}.


From equation (\ref{distributionPropagator}) one can calculate non-equilibrium ensemble averages of any physical phase function $B(t)$:

\begin{equation}
\begin{aligned}
	  \langle B(t) \rangle &= \int_D d\bm{\Gamma}\ B(\bm{\Gamma})\exp[\int_0^{t} ds\ \Omega(S^{-s}\bm{\Gamma})]f(\bm{\Gamma};0)\\
	  &= \langle B(0) \exp[\int_0^{t} ds\ \Omega(S^{-s}\bm{\Gamma})] \rangle_{f(\bm{\Gamma};0)}.
\end{aligned}
\end{equation}

Differentiating the last equation with respect to time we get:
\begin{equation}
\begin{aligned}
 \frac{d\langle B(t) \rangle}{dt}
 &= \int_D d\bm{\Gamma}\ B(\bm{\Gamma}) \Omega(S^{-t}\bm{\Gamma}) f(\bm{\Gamma};t)\\
&= \int_D d\bm{\Gamma}\ B(S^t \bm{\Gamma}) \Omega(\bm{\Gamma})f(\bm{\Gamma};t)\\
&= \langle B(t)\Omega(0) \rangle_{f(\bm{\Gamma};0)}.
\end{aligned}
\end{equation}

If we now integrate it by time, we can write the averages of physical phase functions as:

\begin{equation}
\label{DissipationTheorem}
\langle B(t)\rangle_{f(\bm{\Gamma};0)} =\langle B(0) \rangle_{f(\bm{\Gamma};0)} +\int_0^t ds \langle B(s) \Omega(0) \rangle_{f(\bm{\Gamma};0)}, 
\end{equation}

getting the Dissipation Theorem which states that the nonlinear response of an arbitrary phase variable can be calculated from the time integral of the non-equilibrium transient time correlation function (TTCF) of the phase variable with the dissipation function.


Two simple limits of this theorem can be read of immediately are the  equilibrium case - in which we have no dissipation, so the ensemble averages stay constant and the case in which the external field drives the system out of equilibrium in a linear manner (weak field), equation (\ref{DissipationTheorem}) reduces then to Green-Kubo linear response relation.

\subsubsection{Mixing properties and their relations}

Let's consider a system with at least two zero-mean phase variables $A(\bm{\Gamma})$ and $B(\bm{\Gamma})$. 

\paragraph{Mixing}

A system is said to be mixing if for integrable, reasonably smooth physical phase functions, time correlation functions $\langle A(0) B(t) \rangle_{\mu}$ taken over a stationary distribution $\mu$ factorize in the long time limit:
\begin{equation}
  \lim_{t \to \infty} \langle A(0) B(t)\rangle_{\mu} = \langle A \rangle_{\mu} \langle B \rangle_{\mu}  
\end{equation}

\paragraph{Weak T-mixing}

Weak T-mixing is a direct generalization of mixing for transient rather stationary distributions. Mixing is for correlation functions in systems that have stationary averages of physical phase functions such as equilibrium or steady-state distributions.

If in a system either $\langle A(0) \rangle $ or $ \langle B(t) \rangle = 0, \forall t $, then such a system is called weakly T-mixing if

\begin{equation}
  \lim_{t \to \infty} \langle A(0) B(t) \rangle = 0
\end{equation}

\paragraph{T-mixing}
If a system is weakly T-mixing and the decay of transient correlation takes place at a rate faster than $1/t$ then we say that the system is T-mixing and will be stationary at long times. In other words it's TTCFs must converge to finite values:

\begin{equation}
  \left| \int_0^{\infty} ds \langle A(0) B(s) \rangle \right| = const < \infty 
\end{equation}


\paragraph{$\bm{\Omega T}$-mixing}
We say that a system possesses the property of $\Omega T$ mixing if the integral
\begin{equation}
    \left| \int_0^{\infty} ds \langle B(s) \Omega(0) \rangle \right| = const < \infty
\end{equation}
is bounded from above. This requirement let's us predict that the system will relax either to a non-equilibrium steady state or toward an equilibrium. In other words, it is a \textit{necessary} condition for ensemble averages to be time-independent or stationary at long times.

T-mixing systems are $\Omega T$-mixing, but not all $\Omega T$-mixing are T-mixing. All T-mixing systems must relax to time stationary states in the long time limit.


\subsubsection{Relaxation}

Non-equilibrium system can relax to equilibrium in two ways: conformally and non-conformally.
A conformal system relaxes such that the non-equilibrium distribution is of the form
\begin{equation}
  f(\bm{\Gamma};t)=\exp[-\beta H(\bm{\Gamma})+\lambda(t) g(\bm{\Gamma})] Z^{-1}
\end{equation}
for all times $t$ and the deviation function, $g$, is a constant over the relaxation.
As one might suspect conformal relaxation is an exception rather than the norm in natural relaxation processes.

\subsubsection{Relaxation Theorem}
The Relaxation Theorem states that if an arbitrary initial ensemble of ergodic Hamiltonian systems is in contact with a heat bath and there is a decay of temporal correlations, then the system will at long times, relax to the Maxwell-Boltzmann distribution. Further, this distribution has zero dissipation everywhere in phase space. For such systems no other distribution has zero dissipation everywhere.
\begin{displaymath}
  \lim_{t\to \infty } \Omega (\bm{\Gamma} ;f(\bm{\Gamma} ,t))=0, \forall\ \Gamma
\end{displaymath}

This result is exact arbitrarily far from equilibrium and independent of system size, derviation can be found in \cite{Evans:2241458}.
\subsubsection{Driven systems}

Driven systems are a subcategory of non-equilibrium systems which are subject to an external dissipative field $\bm{F}_e$.
For such systems systems the dissipation function has the form:

\begin{equation}
\label{primaryDissipationFunction}
  \Omega(\bm{\Gamma})\equiv - \beta V \bm{J}(\bm{\Gamma})\cdot \bm{F}_e(s)
\end{equation}
where $V$ is the volume of the system and $\bm{J}$ is the previously defined dissipative flux\footnote{Even though in this definition dissipation is a linear functional of the field, we can always hide higher order dependence under $F_e$}.

If the system that is driven was initially at equilibrium, then equation (\ref{DissipationTheorem}) can be rewritten as:

\begin{equation}
  \langle B(t) \rangle_{f(\bm{\Gamma};0)}=\langle B(0) \rangle_{f(\bm{\Gamma};0)} - V \int_0^t ds\ \langle \beta \bm{J}(0)B(s)\rangle_{f(\bm{\Gamma};0)} \cdot \bm{F}_e
\end{equation}

and at zero field reduces to Green-Kubo expression for the linear response.

%TODO: More on usage page 71

If we consider a simple (fields and dissipation flux taken as scalars), nonequilibrium, thermostatted system of volume, V consisting of charged particles driven by an external field $F_e$ and consider a time-average of the current density along a trajectory as $J_{c,t}= \frac{1}{t}\int_0^t J_c(s)\ ds$, the fluctuation relation of equation (\ref{ESFT}) can then be stated:

\begin{equation}
  \frac{p(J_{c,t}=A\pm dA)}{p(J_{c,t}=-A\pm dA)}= \exp[A \beta F_e V t]
\end{equation}

From this equation, one can see that as the system size or time of observation is increased, the relative probability of observing positive to negative current density increases exponentially so the current density has a definite sign and the second law of thermodynamics is retrieved. In obtaining there results, nothing is assumed about the form of the distribution of current density (it deos not have to be Gaussian). Moreover, in the weak field limit, the rate of entropy production, $\dot{S}$, is given by linear irreversible thermodynamics as $\dot{S} \equiv \sum \langle J_i \rangle V X_i/T$ where the sum is over the product of all conjugate thermodynamic fluxes, $J_i$ and thermodynamics forces, $X_i$, divided by the temperature of the system, $T$. The relation stands out simply as:
\begin{equation}
  \lim_{F_e \to 0} \dot{S}(t) = k_B \langle \Omega(t) \rangle.
\end{equation}
The difference at high fields is because the temperature that appears in the dissipation function is that which the system would relax to if the fields were removed rather than any non-equilibrium system temperature observed with the field on\cite{Evans:2241458}. The change in entropy for a process will be similarly related to the time integral of the dissipation
\begin{equation}
  \lim_{F_e \to 0} \Delta S= k_B \langle \Omega_t \rangle.
\end{equation}

\subsubsection{Non-equilibrium Steady States}

From equation (\ref{StationaryStateDef}) we see that stationarity of a system implies that it's physical properties do not vary in time. This can be understood in the sense of all times or sufficiently late times, however stationarity does not imply that the distribution function is stationary.
%TODO: need to add reference
The time independent values of physical properties, can however, be dependent on the initial phase $\bm{\Gamma}$, if they are not; we call it peNESS (physically ergodic non-equilibrium steady state):

\begin{equation}
      \lim_{t \to \infty} \langle A(t) \rangle_0 = \lim_{t \to \infty} \frac{1}{t} \int_0^t ds\ A(S^s \bm{\Gamma})
\end{equation}
 where $\langle ... \rangle_0$ denotes an ensemble over the initial time $t=0$ and ensemble $f(\bm{\Gamma};0)$.
Contrary to intuition, not all NESSs are physically ergodic. An example is the Rayleigh-Benard instability which occurs in a system with fixed boundary conditions and fixed geometry where a system might develop to a fixed number of rolls (two, four etc.) and persist in it indefinitely\cite{Evans:2241458}.
\subsection{Generalized Crooks fluctuation theorem}

Crooks fluctuation theorem together with Jarzynski equality were originally developed for determining the difference in free energy of canonical equilibrium states from experimental information taken from non-equilibrium paths that connects two equilibrium states. 

%TODO: Intro to GCFT page 156
In order to establish a connection with Crooks fluctuation theorem, we'll need another definition of so called \textit{generalized dimensionless "work"} $\Delta X_{\tau}(\bm{\Gamma})$ for a trajectory of duration $\tau$ originating from the phase point $\bm{\Gamma}$ as

\begin{equation}
\begin{aligned}
\label{GeneralizedWorkDef}
  \exp[\Delta X_{\tau}(\bm{\Gamma})] &\equiv \lim_{d V_{\bm{\Gamma}} \to 0} \frac{p_{eq,1} (d V_{\bm{\Gamma}};0)Z(\lambda_1)}{p_{eq,2} (d V_{\bm{\Gamma}_{\tau}};0)Z(\lambda_2)} \\
  &= \frac{f_{eq,1}(\bm{\Gamma}) d\bm{\Gamma} Z(\lambda_1)}{f_{eq,2}(\bm{\Gamma}_{\tau}) d(\bm{\Gamma}_{\tau}) Z(\lambda_2)}
\end{aligned}
\end{equation}

where $Z(\lambda_i)$ is the partition function for the system and is just a normalization factor for the equilibrium function $f_{eq}(\bm{\Gamma}) =\exp[F(\bm{\Gamma})]/Z$, where$F(\bm{\Gamma})$ is some single-valued phase function. After time $\tau$ the system ends it's parametric change in $\lambda$, however the system is \textit{not} in equilibrium. That is $f(\bm{\Gamma};0)=f_{eq,1}(\bm{\Gamma})$ but $f(\bm{\Gamma};\tau)\neq f_{eq,2}(\bm{\Gamma})$ in general, since relaxation to complete thermal equilibrium cannot take place in finite time.
It can be shown that generalized work defined this way is in fact a state-function when evaluated along quasi-static paths.

The Generalized Crooks Fluctuation Theorem (GCFT) considers probability $p_{eq,f}(\Delta X_t = B \pm dB)$ of observing values of $\Delta X_t$ in the range $B\pm dB$ for forward trajectories starting from the initial equilibrium distribution 1, $f_1(\bm{\Gamma};0)=f_{eq,1}(\bm{\Gamma})$, and the probability $p_{eq,r}(\Delta X_t = -B \mp dB)$ of observing $\Delta X_t$ in the range $ -B\mp dB$ for reverse trajectories byt starting from the equilibrium distribution given by $f_{eq,2}(\bm{\Gamma})$ of system 2.

The probability that the phase variable $\Delta X_{\tau}$ takes the value $B$ for a forward evolved trajectories is given by
\begin{equation}
  p_{eq,1}(\Delta X_{\tau,f}=B\pm dB) = \int_{\Delta X_{\tau,f}=B\pm dB} d\bm{\Gamma} f_{eq,1}(\bm{\Gamma})
\end{equation}

Analogously, the probability of particular values for backward evolved trajectories starting from $f_{eq,2}(\bm{\Gamma})$ is given by

\begin{equation}
  p_{eq,2}(\Delta X_{\tau,r}=-B\mp dB) = \int_{\Delta X_{\tau,r}=-B\mp dB} d\bm{\Gamma} f_{eq,2}(\bm{\Gamma})
\end{equation}
Now looking at the ratio of those probabilities we get (to simplify the notion we will suppress $\pm B$ and instead use $+$/$-$ in the superscript of $\Delta X$)
\begin{equation}
\begin{aligned}
  \frac{p_{eq,1}(\Delta X_{\tau,f}^+)}{p_{eq,2}(\Delta X_{\tau,r}^-)}
= \frac{\int_{\Delta X_{\tau,f}^+(\bm{\Gamma})} d\bm{\Gamma} f_{eq,1}(\bm{\Gamma})}{\int_{\Delta X_{\tau,r}^-(\bm{\Gamma})} d\bm{\Gamma} f_{eq,2}(\bm{\Gamma})}
\end{aligned}
\end{equation}
Now using the definition of generalized work, equation (\ref{GeneralizedWorkDef}), two times first get $f_{eq,2}(\bm{\Gamma})d\bm{\Gamma}= \exp[\Delta X_{r,\tau}(\bm{\Gamma})] f_{eq,1}(S^T \bm{\Gamma}) d(S^T \bm{\Gamma}) Z(\lambda_1)/Z(\lambda_2) $ and also, by inserting $\bm{\Gamma} \to M^T S^{\tau} \bm{\Gamma}$, we see that $\Delta X_{\tau,r}(\bm{\Gamma}) = -\Delta X_{\tau,f}(M^T S^{\tau} \bm{\Gamma}) $ or $\Delta X_{\tau,r}^-(\bm{\Gamma}) = \Delta X_{\tau,f}^+(M^T S^{\tau} \bm{\Gamma}) $ in the simplified notion. Using those two results we perform the transformations:

\begin{equation}
\begin{aligned}
\frac{p_{eq,1}(\Delta X_{\tau,f}^+)}{p_{eq,2}(\Delta X_{\tau,r}^-)}
&=\frac{\int_{\Delta X_{\tau,f}^+(\bm{\Gamma})} d\bm{\Gamma} f_{eq,1}(\bm{\Gamma})}{\int_{\Delta X_{\tau,r}^-(\bm{\Gamma})} d\bm{\Gamma} f_{eq,2}(\bm{\Gamma})} \\
&= \frac{\int_{\Delta X_{\tau,f}^+(\bm{\Gamma})} d\bm{\Gamma} f_{eq,1}(\bm{\Gamma})Z(\lambda_2)/Z(\lambda_1)}{\int_{\Delta X_{\tau,f}^+(M^T S^{\tau}\bm{\Gamma})} \exp[-\Delta X_{\tau,f}^+(M^T S^{\tau}\bm{\Gamma})] d(M^T S^{\tau}\bm{\Gamma}) f_{eq,1}(M^T S^{\tau}\bm{\Gamma})} \\
&=\frac{\int_{\Delta X_{\tau,f}^+(\bm{\Gamma})} d\bm{\Gamma} f_{eq,1}(\bm{\Gamma}) Z(\lambda_2)/Z(\lambda_1)}{\int_{\Delta X_{\tau,f}^+(\bm{\Gamma}')} d\bm{\Gamma}' \exp[-\Delta X_{\tau,f}^+(\bm{\Gamma}')] f_{eq,1}(\bm{\Gamma}')} \\
&= \exp[B] \frac{Z(\lambda_2)}{Z(\lambda_1)}
\end{aligned}
\end{equation}

%TODO: Not sure if that would hold for $\tau \neq 0$

Rewriting this again in full notion:
\begin{equation}
\label{GCFR}
\frac{p_{eq,1}(\Delta X_{\tau,f}=B\pm dB)}{p_{eq,2}(\Delta X_{\tau,r}=-B\mp dB)}= \exp[B] \frac{Z(\lambda_2)}{Z(\lambda_1)}
\end{equation}
we obtain the generalized Crooks fluctuation relation (GCFR).

In order to use GCFT we specialize the obtained result to an actual statistical mechanical ensemble and system of dynamics, obtaining the canonical forms of CFT between initial and final equilibrium states with the same values of temperature, volume and number of particles (T,V,N). 

Referring to equation (\ref{ThermostattedEq}), the equilibrium distribution function $f(\bm{\Gamma};0)$, the related free energy $F(\lambda)$ and partition function $Z$ are given by

\begin{equation}
\begin{aligned}
  f(\bm{\Gamma};0)=Z^{-1} \exp[-\beta H(\bm{\Gamma},0)],\\
  F(\lambda)\equiv - k_B T \ln Z(\lambda) = -k_B T \ln[\int d\bm{\Gamma} \exp[-\beta H(\bm{\Gamma},\lambda)]].
\end{aligned}
\end{equation}

The Hamiltonian is varied parametrically from $\lambda_1 =\lambda(0)$ to the final, unique equilibrium state\footnote{To which it will relax thanks to the property of T-mixing} $\lambda_2 = \lambda(\tau)$. In coupled system of equation (\ref{ThermostattedEq} ), the phase space volume changes and so the work with the Hamiltonian $H_E$ for the extended system becomes:

\begin{equation}
\begin{aligned}
  \Delta X_\tau &= \beta ( H_E(S^\tau \bm{\Gamma}, \lambda(\tau))-H_E(\bm{\Gamma},\lambda(0))) + \ln\left[\frac{d\bm{\Gamma}}{d(S^\tau \bm{\Gamma})}\right]\\
  &=\beta ( H_E(S^\tau \bm{\Gamma}, \lambda(\tau))-H_E(\bm{\Gamma},\lambda(0))) + \int_0^\tau ds\ \Lambda(S^s\bm{\Gamma})\\
  &=\beta ( H_E(S^\tau \bm{\Gamma}, \lambda(\tau))-H_E(\bm{\Gamma},\lambda(0)) +\Delta Q_\tau) \\
  &=\beta \Delta W_\tau
\end{aligned}
\end{equation}

The generalized dimensionless "work" became identifiable as $\beta$ times the work performed over a period of time $\tau$:

\begin{equation}
\label{CFT}
  \frac{p_1(\Delta W_\tau =W)}{p_2(\Delta W_\tau = -W)} = exp[\beta (W - \Delta F)],
\end{equation}
thus obtaining the standard Crooks fluctuation theorem.

%TODO: Improvement of GCFR?
%TODO: More discussion needed - page 168
\subsection{Jarzynski Equality}

In ordinary statistical physics when transitions between two equilibrium states are performed infinitely slowly along some path between the initial point $A$ and the final point $B$, then the total work $W$ performed on such a system is equal to the Helmholtz free energy difference $\Delta F$ between the initial and final configurations. However, this is not the case when non-equilibrium transitions are considered. In fact, on average the work performed on the system will exceed Helmholtz free energy $ \langle  W \rangle \geq \Delta F $ and the difference will be equal to the dissipated energy, associated with increase of entropy during an irreversible process.

In 1996, Christopher Jarzynski\cite{Jarzynski:1997uj} derived a very useful equality for the equilibrium free energy differences between two configuration of a system in terms of an ensemble finite-time measurements of the work performed during parametric switching in between those two configurations.

\begin{equation}
\label{JarzynskiInequality}
  \langle \exp(-\beta W) \rangle = \exp(-\beta  \Delta F)
\end{equation}


Having already derived GCFR we will will use it to derive Jarzynski Equality also in it's generalized form, which expresses the free energy difference between two equilibrium states in terms of an average over irreversible paths.
In fact a generalized Jarzynski equality (GJE) follows from:

\begin{equation}
\begin{aligned}
  \langle \exp[-\Delta X_{\tau}(\bm{\Gamma})] \rangle_{eq,1} 
  &= \int_{-\infty}^{\infty} dB\ p_f(\Delta X_\tau =B)\exp[-B]\\
 &= \int_{-\infty}^{\infty} dB\ p_r(\Delta X_\tau =-B)\frac{Z(\lambda_2)}{Z(\lambda_1)}\\
 &=\frac{Z(\lambda_2)}{Z(\lambda_1)}
\end{aligned}
\end{equation}

where the brackets $\langle ... \rangle_{eq,1}$ denote an equilibrium ensemble average over the initial equilibrium distribution.

The usual practice is is use the inequality $e^x \geq 1+x $ to rewrite it in a form of an inequality:

\begin{equation}
\begin{aligned}
  \frac{Z(\lambda_2)}{Z(\lambda_1)} &=\langle \exp[-\Delta X_\tau ]\rangle_1 \\
  &=exp[-\langle\Delta X_\tau \rangle_1]\langle \exp[-\Delta X_\tau + \langle \Delta X_\tau \rangle_1]\rangle \\
  &\geq exp[-\langle\Delta X_\tau \rangle_1]\langle 1- \Delta X_\tau + \langle \Delta X_\tau \rangle_1 \rangle \\
  &=exp[-\langle\Delta X_\tau \rangle_1]
\end{aligned}
\end{equation}

or taking the logarithm

\begin{equation}
  \langle \Delta X_\tau \rangle \geq \ln\left[\frac{Z(\lambda_1)}{Z(\lambda_2)}\right]=\beta \Delta F_{21}
\end{equation}

here the right side is the free energy difference. Now it's time to specialize our generalized work with a specific definition:
\begin{equation}
  \Delta X = \beta \int_0^\tau ds\ W(s)
\end{equation}
where $W$ denotes the work. This inequality implies $\Delta W_{21} \geq \Delta F_{21} $, so the minimum work is expended if the path is reversible or quasi-static, in which case the work is, in fact, the difference in the free energies.% divided by $k_B T$.

If we choose the second equilibrium to be in fact our first equilibrium ($Z_1/Z_2=1$), therefore inducing a closed cycle, then the inequality implies

\begin{equation}
\label{CyclicInequalityForGeneralizedWork}
  \oint ds \langle X(s) \rangle = \oint \langle dS \rangle \geq 0
\end{equation}

i.e. the ensemble average of the cyclic integral of the generalized work is nonnegative.
Although it's appearance is similar to Clausius inequality, for the heat we have to complete many cycles until the system settles into a periodic response to the cyclic protocol before we can apply the cyclic integral of the heat. Not all systems do settle into a cyclic response. The reason for this disrepancy is that after the cycle is complete no more work is being done, but the long relaxation process of course includes heat exchange. 

\section{Application to self-replication and adaptation}
\label{CrooksApplications}
Now we wish to zoom out a bit from our theoretical considerations about the foundations of fluctuation theorems and just take them as a fact in some recent and interesting applications\cite{Perunov:2016hl}\cite{England:2013ed}. The main new truth obtained from them is that we can partially follow, what we consider "microscopic trajectories", but those are not the real microscopic trajectories of fundamental particles, thus dissipation of heat occurs along the way.

We proceed by switching to a stochastic description in which the equation (\ref{CFT}) takes the form
%Check why ensemble average
\begin{equation}
\label{MicroscopicIrreversibility}
\frac{\pi (j\to i;\tau )}{\pi (i\to j;\tau )}= \langle \exp[-\beta  \Delta Q_{i\to j}^{\tau}] \rangle_{i \to j},
\end{equation}

where $\pi$'s are the probability distributions of transitions over trajectories, during time $\tau$ either from $ j\to i$ or $ i \to j $ and $Q$ is the dissipated heat.

Now let's define two macroscopic states denoted by I and II.
We can then define the probabilities of transitions from macrostate I to macrostate II with the use of conditional probabilities\footnote{Note that we can do that, because we take into account what happens at the microscopic level}:

\begin{equation}
  \pi (I\to II)=\int_{II} d j \int_I d i\ \pi(i\to j)p(i|I)
\end{equation}
and
\begin{equation}
  \pi (II\to I)=\int_{I} d i \int_{II} d j\ \pi(j\to i)p(j|II)
\end{equation}

Now let's investigate the ratio

\begin{equation}
\begin{aligned}
\label{MacrostatesPRatio2}
  \frac{\pi(II \to I)}{\pi(I \to II)} &= \frac{\int_{I} d i \int_{II} d j\ \pi(j\to i)\frac{p(j|II)}{p(i|I)}p(i|I)}{\int_{II} d j \int_I d i\ \pi(i\to j)p(i|I)}\\
  &= \frac{\int_{I} d i \int_{II} d j\ \pi(i\to j) \langle \exp[- \beta \Delta Q_{i\to j}^{\tau}] \rangle_{i \to j}  \frac{p(j|II)}{p(i|I)}p(i|I)}{\int_{II} d j \int_I d i\ \pi(i\to j) p(i|I)}\\
  &=\langle \langle e^{- \beta \Delta Q_{i\to j}^{\tau}} \rangle_{i \to j} e^{\ln[\frac{p(j|II)}{p(i|I)}]} \rangle_{I \to II}
\end{aligned}
\end{equation}

where in the last step we made use of equation (\ref{MicroscopicIrreversibility}) and  $\langle ... \rangle_{I \to II}$ denotes an average over all paths from some microstatei in the initial ensemble I to some microstate j in the final ensembleII, with each path weighted by its likelihood.

One can rewrite the equation (\ref{MacrostatesPRatio2}) as 

\begin{equation}
\label{MacrostatesPRatio2b}
  \frac{\pi(II \to I)}{\pi(I \to II)} 
  =\langle e^{- \beta \Delta Q_{i\to j}^{\tau} + \ln[\frac{p(j|II)}{p(i|I)}]} \rangle_{I \to II}
\end{equation}

remembering that $\Delta Q_{i \to j}$ contains a path ensemble average,
and compare it with equation (\ref{MacrostatesPRatio}) to see the essential difference between those two equations, namely the partial knowledge about microscopic trajectories and their dissipated heat.

Now moving the left side of (\ref{MacrostatesPRatio2b}) to the right side and under the ensemble one gets

\begin{equation}
\langle  e^{- \beta \Delta Q_{i\to j}^{\tau}} e^{\ln[\frac{p(j|II)}{p(i|I)}]} e^{-\ln[\frac{\pi(II \to I)}{\pi(I \to II)}]} \rangle_{I \to II}
\end{equation}

which by making use of $e^x \geq 1+x$ reduces to

\begin{equation}
  \beta \langle\Delta Q_{i\to j}^{\tau} \rangle_{I \to II}+\langle \ln[\frac{p(i|I)}{p(j|II)}]  \rangle_{I \to II}+ \ln{\frac{\pi(II \to I)}{\pi(I \to II)}} \geq 0.
\end{equation}

The second term can now be identified with Shannon entropy between two macroscopic states $\Delta S_{int} = S_{II}- S_{I}$ obtaining


\begin{equation}
\label{SecondLawII}
  \beta \langle\Delta Q_{i\to j}^{\tau} \rangle_{I \to II} + \ln{\frac{\pi(II \to I)}{\pi(I \to II)}} +\Delta S_{int}  \geq 0.
\end{equation}

This is a very general result holds for wide range of transitions between the coarse-grained starting and ending states and has relevance to the known Landauer bound for heat generated by the erasure of a bit of information \cite{England:2013ed}.

\subsection{Self-replication}
The obtained result can be applied to a simple model of self-replicators. Let's suppose we have a master equation for $n \gg 1$ governing the population

\begin{equation}
  \dot{p}_n(t)=g\ n\left(p_{n-1}(t)-p_n(t)\right)-\delta\ n\left(p_n(t)-p_{n+1}(t)\right)
\end{equation}

where $p_n(t)$ is the probability of having a population of $n$ at time $t$ with grow rate $g$ and decay rate $\delta $.
If we now connect the state of living with macrostate II and the state dead state with macrostate I, then naturally we assign $\pi (I \to II) = g\ \Delta t$ and $\pi (II \to I) = \delta\ \Delta t$ for some time $\Delta t$.

The equation then (\ref{SecondLawII}) dictates

\begin{equation}
  \Delta S_{int} + \beta \langle \Delta Q_{i\to j}^{\tau}  \rangle_{I \to II} \geq \ln{\frac{g}{\delta}}.
\end{equation}

which can interpreted as a general bound on self replication.
An important thing to notice here is that the $\Delta S_{int}$ is expected to be negative, because the self-replicator exists in non-equilibrium, living state.

If one fixes all the terms other than the growth rate, than one can get a bound on the growth rate, by simple algebraic manipulation

\begin{equation}
  g \leq g_{max}=\delta \exp[ \Delta S_{int} + \beta \langle \Delta Q_{i\to j}^{\tau}  \rangle_{I \to II}].
\end{equation}

Most general observation that can be made from this equation is that  in order for the growth rate $g$ to exceed the die rate $\delta $ the negative internal entropy change must be paid by the (strictly larger) dissipated heat. This dissipated energy in case of a self-replicator can have two sources: it is either stored in the reactants out of which the replicator gets built or from work done on the system by some external driving field, such as through the absorption of light.

Another comment can be made considering two self-replicators with the same entropy change $\Delta S_{int}$ and die rate $\delta$, in this scenario we see, that the one with larger heat dissipation will replicate faster.
On the other hand an alternative route is also available by increasing the rate at which the self-replicator degrades $\delta $ and keeping the complexity inner complexity ($\Delta S_{int}$) low.

\subsection{Traversal of energy landscape }

Let's now consider a case of driven thermostatted system with two possible target macrostates $II$, $III$ and we are now interested in the propability ratio between those two. From equation (\ref{MacrostatesPRatio2b}) we  get

\begin{equation}
  \ln \frac{\pi(I \to II)}{\pi(I \to III)} = \ln \frac{\pi(II \to I)}{\pi(III \to I)}- ln \frac{ \langle e^{-\beta \Delta Q_{i \to j} + \ln \frac{p_f^{II}}{p_i^{I}}} \rangle_{I \to II}}{ \langle e^{-\beta \Delta Q_{i \to k} + \ln \frac{p_f^{III}}{p_i^{I}}} \rangle_{I \to III}}
\end{equation}

where the initial and final microstates where noted by $p_s$ and $p_f$.
Because the system is driven and energy conserved, the work done on the system must go either to the heat or the systems hamiltonian:
\begin{equation}
  W_{i \to j}=  \Delta Q_{i \to j} + H_*- H_I
\end{equation}

where the $*$ is the final state indicator, here either $II$ or $III$.
If we now assume that the system is driven for a long time, we might neglect the correlations between the initial and final states and the work, giving us:

\begin{equation}
  \ln \frac{\pi(I \to II)}{\pi(I \to III)} = \ln \frac{\pi(II \to I)}{\pi(III \to I)}- ln \frac{ \langle e^{\beta (H_{II}-H_I) + \ln \frac{p_f^{II}}{p_i^{I}}} \rangle_{I \to II}}{ \langle e^{\beta (H_{III}-H_I) + \ln \frac{p_f^{III}}{p_i^{I}}} \rangle_{I \to III}}-ln \frac{\langle e^{ - \beta W} \rangle_{I \to II}}{\langle e^{ - \beta W} \rangle_{I \to III}}
\end{equation}

The Hamiltonian can be obtained from the underlying equilibrium distributions $p_{eq}= e^{-\beta H_*}/Z_{eq}^*$ by

\begin{equation}
  H_*-H_I= -\beta^{-1}( \ln{p_{eq}^* Z_{eq}^*} -  \ln{p_{eq}^I Z_{eq}^I})= \beta^{-1}( \ln{\frac{p_{eq}^I}{p_{eq}^*}} + \ln{\frac{Z_{eq}^I}{Z_{eq}^*}})
\end{equation}

which after assuming that the initial distribution was at equilibrium, leaves us with 

\begin{equation}
  \ln \frac{\pi(I \to II)}{\pi(I \to III)} =
  \ln \frac{\pi(II \to I)}{\pi(III \to I)} - ln \frac{ \langle \frac{p_{f}^{II}}{p_{eq}^{II}}  \rangle_{II}}{ \langle \frac{p_{f}^{III}}{p_{eq}^{III}}  \rangle_{III} }
  -ln \frac{\langle e^{ - \beta W} \rangle_{I \to II}}{\langle e^{ - \beta W} \rangle_{I \to III}} + \ln{\frac{Z_{eq}^{II}}{Z_{eq}^{III} }}
\end{equation}

One can notice that the second term will be zero if the final distributions are equilibrium distributions. Since free energy is defined as $F^* = -\beta \ln Z_{eq}^*$, we can combine the last two introducing a term called dissipated work, defined by

\begin{equation}
  W_d =W - Z_{eq}^* + Z_{eq}^I
\end{equation}

thus obtaining

\begin{equation}
  \ln \frac{\pi(I \to II)}{\pi(I \to III)} =
  \ln \frac{\pi(II \to I)}{\pi(III \to I)} - ln \frac{ \langle \frac{p_{f}^{II}}{p_{eq}^{II}}  \rangle_{II}}{ \langle \frac{p_{f}^{III}}{p_{eq}^{III}}  \rangle_{III} }
  -ln \frac{\langle e^{ - \beta W_d} \rangle_{I \to II}}{\langle e^{ - \beta W_d} \rangle_{I \to III}} 
\end{equation}

Now we might try to interprate each of those terms, the intuitive meaning of the first one is the fact that more likely are the states from which one can come back. The last term on the other hand might be expanded with the use of cumulant expansion
\begin{equation}
  -\ln \langle \exp(-\beta W_d)\rangle= \beta \langle W_d \rangle - \Phi
\end{equation}

where $\Phi$ holds all higher order terms of the cumulant expansion. From the convexity of the exponential function and Jensen's inequality one might check\footnote{$e^{-\Psi + \Phi}=\langle \exp(-\beta W_d) \rangle \geq \exp(-\beta \langle W_d \rangle)= e^{-\Psi}$} that we must have $\Phi \geq 0$. Thus, $\Phi$ can be thought of as a correction due to the dispersion of the dissipated work distribution about the average $\beta \langle W_d \rangle $ that gives the heaviest weight to the leftward tail of the work distribution\cite{Jarzynski:2006cq}.

One might now argue that averaged dissipated heat $\langle W_d \rangle $ can be connected with particle drift through oscillatory energy barriers (details depending on the model), this was in fact recently demonstrated by Nikolay Perunov et al\cite{Perunov:2016hl}.

We have now seen how recent progress in microscale non-equilibrium statistical physics has allowed us to make plausible predictions about inanimate matter. Indeed the fruitfulness of this approach were noticed by other groups who seek to further generalize it - this will be the topic of next paragraph.

\section{Search for a unifying principle}
\label{UnifyingPrinciple}
The search for variational or extremization principles in physics has a long history of success. In classical mechanics, one finds the equations of motion from Lagrangian formalisms with the principle of least action. In thermodynamics and statistical physics of equilibrium state the principle of maximum entropy yields the true equilibrium states of a given system. 

In 1912, Ehrenfest was the first who asked whether such a principle for a yet unknown function could exist for non-equilibrium steady states \cite{Dewar:2014ek}.

This approach has also gained a lot of criticism.
According to Kondepudi \cite{Kondepudi:1141550}, and to Grandy\cite{Grandy:1135724}, there is no general rule that provides an extremum principle that governs the evolution of a far-from-equilibrium system to a steady state. 

There seems to be a theoretical relationship between maximum irreversibility and dynamic stability, a link suggested here by the fact that MaxEnt/MaxEP predicts the same dissipation functional as Malkus's instability criterion in shear turbulence\cite{Dewar:2014ek}.

\subsection{Rayleigh's insight}
Studying jets of water from a nozzle, Rayleigh \cite{Rayleigh:1878hb} noted that when a jet is in a state of conditionally stable dynamical structure, the mode of fluctuation most likely to grow to its full extent and lead to another state of conditionally stable dynamical structure is the one with the fastest growth rate. In other words, a jet can settle into a conditionally stable state, but it is likely to suffer fluctuation so as to pass to another, less unstable, conditionally stable state. He used like reasoning in a study of Benard convection\cite{Rayleigh:1916fa}. These physically lucid considerations of Rayleigh seem to contain the heart of the distinction between the principles of minimum and maximum rates of dissipation of energy and entropy production, which have been developed in the course of physical investigations on so-called MaxEP principle by later authors.

\subsection{MaxEP principle}

Recently, a common working formulation of the maximum entropy production (MaxEP, sometimes called MEP) principle surfaced stating roughly that \textit{for systems admitting a spectrum of possible steady states, MaxEP says that the system is most likely to be found in steady state  with the greatest entropy production}

The conjecture of MaxEP has shown some promising (but controversial) success in studies of planetary climates \cite{Paltridge:2007jf}, fluid turbulence \cite{Ozawa:2003jt} \cite{MALKUS:2003ix}, crystal growth morphology, biological adaptation as well as earthquake dynamics. %TODO: Fill the references from chapter 3, Beyond the Second Law
The main reason that drove controversies around those successes has been an \textit{ad hoc} and unsystematic manner in which MaxEP was applied.

For example, the earliest successes of MaxEP applied to Earth's climate were based on a 2-zone model where the energy balance and temperatures were obtained through maximization of entropy production (EP) associated with meridional heat transport in the atmosphere and the oceans, completely ignoring the dominant part of the total EP coming from radiative EP. At the same time general circulation models (GCM), found no extremum in EP. MaxEP was also not found in phenomenological models of heat flow in plasma/fluid system \cite{Kawazura:2010dy}, where maximum as well as minimum was observed depending on how the system is driven. Nevertheless, possible adjustments to MaxEP principle are still being researched.

\subsubsection{Relation to MinEP}
For linear, near-equilibrium systems that only admit a single steady state, MinEP says that all of the system's transient states have a higher entropy production than the steady state. A transient state is a temporary state that is not a steady state. MinEP compares steady states with non-steady states.

For some yet-to-be-determined class of non-linear, far-from-equilibrium systems that admit a continuum of possible steady states, MaxEP says that the system is most likely to be found in the steady state with the greatest entropy production. MaxEP compares steady states to other steady states, but says nothing about transient states.

%TODO: Cite Nathaniel

\subsubsection{Extrema of the Dissipation Function and MaxEP}
The dissipation function is similar to the entropy production, and although it is not directly connected to a state function, the various fluctuation theorems provide exact, non-equilibrium relations. Given this similarity it is interesting to consider whether there exists a principle akin to MaxEnt (maximum entropy) for equilibrium systems, and MaxEP (maximum entropy production rate) for non-equilibrium systems. There were several papers on the topic, including Williams and Evans \cite{Williams:2008ft}, concluding that MaxEP cannot be applied rigorously to non-equilibrium systems in general, as the distribution function at any time (including steady state) is not just a function of the dissipation at that time. However it might provide a good approximation in some cases.

Considering a system that is driven from an initially equilibrium state to a steady-state, one finds that the ensemble average of the instantaneous dissipation function stays positive, and that the average of the total dissipation function will approach infinity at long times. Considering equation (\ref{DissipationTheorem}) with $B=\Omega$, one can see \textit{then if} the autocorrelation function $\langle \Omega(\bm{\Gamma}(0))\Omega(\bm{\Gamma}(t))\rangle$ decays monotonically with time, then the value of the ensemble average of the instantaneous dissipation funciton, $\langle \Omega(t) \rangle$ will be higher when it reaches its steady state than for any other state it passes through. Therefore the system would find itself in a steady-state that maximises the dissipation function (rate of entropy production).

However this is a special case, under the assumption of a monotonous autocorrelation function of the dissipation. Some numerical studies has been performed to study the behaviour of the instantaneous dissipation function more generally; examining whether it is a maximum in the steady state or if a transient state has a higher average instantaneous dissipation value.

In fact it was shown in \cite{Brookes:2011hu} through numerical simulations, that the dissipation average peaks before dropping to a steady state for most field values. Therefore it is clear that the instantaneous dissipation is not a maximum in the steady state - the system evolves through unstable (transient) maximum. For stronger fields, the steady state reaches a higher instantaneous dissipation which is also maximum.

One should note here that in the light of formulation of MaxEP from the previous paragraph this result doesn't prove or disprove the MaxEP principle as it compares steady states to transient states, instead of steady states to steady states. One might conclude however that the supposed theorem of maximum instantaneous dissipation for steady states does not generally apply.

In order to provide further information on the behaviour of the dissipation, one should consider a system where multiple steady state solutions are known to exist. One such model was investigated by Zhang\cite{Zhang:2001jc} who considered heat flow in a one-dimensional lattice. The two possible steady states exist - a soliton or a diffusive heat flow - and depend on the initial conditions and the field strength.
For a given field strength, there is a certain set of trajectories from which a soliton emerge spontaneously. There's also a critical value of the field strength after which the probability of forming a soliton is equal to unity. This chaos-soliton transition becomes sharper as the size of the system (number of particles) is increased. 
One might think that it may very well be that after increased simulation time the transition becomes sharper as well, but it turns out for zero-field there exists a set of initial conditions (of zero measure) that also forms a soliton. This peculiar points of the phase space form a basin of attraction which grows after the external field is increased. This non-uniqness of the steady state stands in contrast to other 1D and 2D  numerical simulations like \cite{Maeda:1995bj}.

Those examples might suggest that the strong conjecture of MaxEP might not hold for all steady state or that the field strength should also be considered a constrain of the system.
Perhaps, MaxEP requires the external forcing to be sufficiently large that low entropy states are unstable. It is clear that the results will depend on the constraints imposed, and therefore the problem could be reformulated as a problem in identification of the appropriate constraints.

It is difficult to study MaxEP numerically at the microscopic level, the main reason for that is because systems that generate multiple steady states, such as convection and turbulent flow, are computationally very expensive. One would then obtain MaxEP as an objective way of finding the appropriate constrains.

One should perhaps mention, before committing to such enterprise, that there exist discouraging examples. First is seen in the derivation of the dissipation theorem for driven system from an extremum principle \cite{Evans:1985br}. In that case the number of constraints (chaining constrains of the dissipative flux for each time step) required in order to obtain the exact answer turned out to be infinite. 
Second is the analysis of family of dissipation functionals of the form $f= D(D_v/D_m)^n$ and $f= D_m(D_v/D_m)^n$ for $n\geq 0$, involving the total dissipation $D$, dissipation in the mean flow $D_m$, and dissipation in the fluctuating flow $D_v$. Within this family Kerswell \cite{Kerswell:2002bw} was unable to identify a universal dissipation functional that applies to all flow problems.

\subsubsection{MaxEnt based formulations of MaxEP}

Perhaps the most promising interpretation of MaxEP principle, put forward by Dewar\cite{Dewar:775452, Dewar:2009fg, Dewar:2005eo}, assumes that MaxEP is not a physical principle at all. Instead, by analogy to Jaynes MaxEnt it is to be interpreted as an inference method i.e. a method for deducing the most unbiased predictions from an incomplete set of statistical data. 

First step is done by narrowing the scope of validity. We notice that systems that are weakly driven have (neglecting the set of measure zero) only one steady state available.\footnote{Of course, in principle, there is always only one stationary state when our knowledge about the system is full, however, by assumption, in case of strongly driven systems our ignorance is much greater and there is more room for an inference principle.} Therefore there's no room for MaxEP to operate and we instead focus on systems driven strongly in far for equilibrium regime.

Two classic examples of far from equilibrium systems involve \textit{shear turbulence} with Reynolds numbers greater than the critical value necessary for the onset of turbulence and \textit{Rayleigh-Bénard} cell with Rayleigh numbers greater than the critical value necessary for the onset of convection.
In those scenarios both examples exhibit many flow solutions allowed when we apply only a restricted set of stationary conditions rather than the full dynamics.
%TODO: Place for work of Ozawa

Secondly, Dewar introduces information theoretical measure of the distance from equilibrium, or \textit{irreversibility} $I$ (similar to defined in section \ref{IrreversibilityMeasure}), defined in terms of the relative probabilities of forward and reverse fluxes. 
The first demand (dynamical instability) is then reformulated as a strong inequality constraint $I>I_{min}$. 
Then using procedures known from MaxEnt it is shown that $I$ adopts it's maximum possible value under the stationarity constraints.

The final step consists of reinterpretation of $I$ as thermodynamic entropy production. In this derivation of MaxEP, entropy production depends of applied constraints.

In the proceeding section the procedure (which is Dewar's third iteration of the principle) is presented in detail.

\subsection{MaxEP principle as an inference principle}

We will consider a general open system (volume $V$, bondary $\Omega$) which exchanges both matter and energy with it's surroundings. The system may consist of several components.

The presence of fluxes, both within the system, and between the system and its environment is the primary characteristics of the non-equilibrium stationary states. We'll denote the instantaneous value of those fluxes, by the vector $\bm{f}$, which may in principle be infinite. The flux vector $\bm{f}$ may be related to some local density $\rho$ with the use of the continuity equation $\partial \rho / \partial t =  - \nabla \cdot \bm{f} + h$, where $h$ denotes a local source; alternatively (for example in case of Navier-Stokes equations), the components of $\bm{f}$ might themselves be identified with local densities. Macroscopic state of the system is then described by $\bm{f}$ (or $\rho$) with the stationarity condition given by equation (\ref{SteadyStates})

Similarly to the MaxEnt we maximize the relative entropy

\begin{equation}
  H = - \int p(\bm{f})\ln \frac{p(\bm{f})}{q(\bm{f})} d\bm{f}
\end{equation}

with respect to probability distribution function $p(\bm{f})$, subject to given dynamical constraints (assumed relevant dynamics, typically involving a restricted number of stationary conditions) $C$, where $q(\bm{f})$ is a prior p.d.f, with the symmetry $q(\bm{f}) =q(\bm{-f})$ which corresponds to zero flux state $\bm{F}= \int q(\bm{f}) \bm{f} d\bm{f}=0 $.

By Gibbs' inequality, $H \leq 0$ with equality if and only if $p(\bm{f})=q(\bm{f})$.


The constraints represented by $C$ are written in the generic form of functionals of fluxes $\phi_m(\bm{f})$ and labeled by $m$:

\begin{equation}
\label{Constrain1MEP}
  \int p(\bm{f})\phi_m(\bm{f})d\bm{f} =0
\end{equation}
which we can demand without loss of generality to be zero.
We also have the normalization constraint

\begin{equation}
\label{Constrain2MEP}
  \int p(\bm{f})d\bm{f} = 1.
\end{equation}

In order to enforce the multiplicity of stationary states we introduce the \textit{irreversibility} defined by the Kullback-Leibler (KL) divergence of $p(\bm{f})$ and $p(-\bm{f})$:

\begin{equation}
\label{IrreversibilityMeasure}
  I= \int p(\bm{f}) \ln \frac{p(\bm{f})}{p(-\bm{f})} d\bm{f}.
\end{equation}

By Gibbs' inequality, $I \geq 0$ with equality if and only if $p(\bm{f})=p(-\bm{f})$, so that $I=0$ corresponds to the equilibrium state $\bm{F} = \bm{0} $. The irreversibility $I$ is thus a natural information-theoretic measure of the distance from equilibrium, or time-reversal symmetry breaking, since it measure the extent to which $p(\bm{f} ) $ differs from $p(\bm{-f} ) $.

Now we make the first step from the previous paragraph, by demanding that there is a state characterized by minimal irreversibility, $I>I_{min}(C)>0$, the value of which depends on the stationarity conditions $C$ of equation (\ref{Constrain1MEP});.

We assume that those conditions also determine the upper bound $I \leq I_{max}(C)$.

Additionally we introduce a trial mean flux $\bm{F}$ (which is subsequently relaxed) via the auxiliary constraint

\begin{equation}
\label{Constrain3MEP}
  \int p(\bm{f})\bm{f} d\bm{f} = \bm{F}.
\end{equation}

The motivation for introducing $\bm{F}$ this way is that $\bm{F}$ represents a trial estimate of the actual fluxes, $\bm{F}(C)$ selected under C. Introducing $\bm{F}$ in this way allows one to establish an extremal principle whereby $\bm{F}(C)$ is determined by varying the trial solution $\bm{F}$.
This approach is analogous to the way in which equilibrium variational principles (e.g. minimum free energy) can be derived from MaxEnt by enlarging the set of fixed macroscopic variables $X$ to include one or more free unconstrained variables $Y$, then maximizing $S=H_{max}(X,Y)$ with respect to $Y$ with $X$ held fixed.

Under given constraints the MaxEnt solution for $p(\bm{f})$ is given by 

\begin{equation}
\label{MaxEntPDF1}
  p(\bm{f})^* = q( \bm{f} ) Z^{-1} \exp[\bm{\lambda} \cdot \bm{f}+ \bm{\alpha} \cdot \bm{\phi} (\bm{f}) - \mu ( d(\bm{f}) - e^{-d(\bm{f}))}]
\end{equation}

where $d(\bm{f})= \ln{p(\bm{f})/p(-\bm{f})}$, $\phi(\bm{f})$ denotes the vector with components $\phi_m(\bm{f})$, $Z = Z(\lambda, \bm{\alpha},\mu)$ is a normalisation factor (partition function) and $\bm{\lambda}$, $\bm{\alpha}$ and $\mu$ are Lagrange multipliers for (\ref{Constrain3MEP}), (\ref{Constrain1MEP}) and the upper-bound inequality $I<I_{max}$ respectively. The maximized relative entropy is

\begin{equation}
  S(\bm{F}, I_0, C) \equiv H_{max} = \ln Z(\lambda, \bm{\alpha}, \mu) - \bm{\lambda} \cdot \bm{F} + \mu(I_0 -1)
\end{equation}

The next step involves maximizing $S(\bm{F}, I_0, C)$ with respect to the trial flux solution $\bf{F}$ with $I_0$ held fixed.

In the absence of the dynamic instability condition $I>I_{min}(C)$, MaxEnt predicts the basal state $I=I_{min}(C)$, i.e. minimal irreversibility, but when the basal state is excluded by the dynamical instability condition MaxEnt predicts a probability distribution function for which $I=I_{max}(C)$ i.e. maximal irreversibility which is characterized by $\mu =0$ and equation (\ref{MaxEntPDF1}) becomes

\begin{equation}
\label{MaxEntPDF2}
  p(\bm{f})^* = q( \bm{f} ) Z^{-1} \exp[\bm{\lambda} \cdot \bm{f}+ \bm{\alpha} \cdot \bm{\phi} (\bm{f})]
\end{equation}

The intermediate solutions not predicted by this procedure would then correspond to transient states, between equilibrium state and the non-equilibrium stationary state.

The predicted irreversibility measure (equation \ref{IrreversibilityMeasure}) is then given by a functional of $\bf{F}$:
\begin{equation}
  I(\bm{F}) =2\ \lambda(\bm{F}) \bm{F} + 2\ \bm{\alpha(\bm{F})} \Phi^A(\bm{F})
\end{equation}

where 

\begin{equation}
  \Phi^A(\bm{F}) \equiv \frac{1}{2} \int p(\bm{f})[\bm{\phi}(\bm{f})-\bm{\phi}(\bm{-f})]d\bm{f}
\end{equation}

is the expectation value of the anti-symmetric part of $\bm{\phi}(\bm{f})$. One may now notice that if the vector $\bm{\phi}$ is anti-symmetric or symmetric then (after adding the condition \ref{Constrain1MEP}) the $\bm{\Phi}^A =0$, so that

\begin{equation}
\label{SymmetricIrreversible}
  I(\bm{F})=2 \bm{\lambda}(\bm{F})\bm{F}.
\end{equation}

On the other hand when $\bm{\Phi}^A$ displays no pure symmetry then

\begin{equation}
\label{NonSymmetricIrreversible}
  I(\bm{F}) = 2\ \bm{\alpha(\bm{F})} \Phi^A(\bm{F}).
\end{equation}

We conclude that there is no general expression for thermodynamic entropy independent of the constraints.

\subsubsection{Relation to fluctuation theorem}
In his first approaches, Dewar claimed to derive \cite{Dewar:775452, Dewar:2005eo} the fluctuation theorem from MaxEP. The claims in the more recent works are more modest\cite{Dewar:2014ek}- the MaxEP so described, is in fact an approximation to the fluctuation theorem, a conclusion, natural regarding the fact that the fluctuation theorem seems to describe the non-stationary states equally well.

The irreversibility measure $I$ defined above is related to the dissipation function $\Omega_t$ of equation (\ref{Dissipation}). In fact $I=\langle \Omega_t \rangle$ if we interpret the flux vector $\bm{f}$ as a time-average over time $t$. 

\subsubsection{Example application for planetary atmospheres}

In this section we'll describe an example application of MaxEP to planetary climates, therefore justifying ad-hoc hypotheses made in \cite{Paltridge:2007jf}\cite{Lorenz:J80tzZkl}.

The constraints in this model 1D box model are given by a simple radiative balance between total incoming short-wave (SW) irradiance and the total outgoing long-wave (LW) irradiance:

\begin{equation}
\label{BalanceEq}
  \sum_i F_{SW,i}= \sum_i F_{LW,i}
\end{equation}
  
  where the sum goes over all latitudinal zones. The meridional heat fluxes are identified as $\bm{f}=\{f_i\}$ where $f_i$ is the flux from zone $i-1$ to zone $i$. 

The local (for each zone) equation has then the form
\begin{equation}
  F_{SW,i}- F_{LW,i} + \Delta F_i = 0
\end{equation}

where $\Delta F_i = F_i - F_{i+1}$ and $\int p(\bm{f})\bm{f} d\bm{f}=\bm{F}=\{F_i\}$. 

The basal equilibrium state corresponds then to radiative equilibrium $F_{SW,i}= F_{LW,i}$ with $\bm{F}=\bm{0}$ and $I=I_{min}$.

Now, the equation (\ref{BalanceEq}) can be written as a condition

\begin{equation}
  \int p(\bm{f})\phi_1(\bm{f})d\bm{f} =0
\end{equation}

with antisymmetric functional $\phi_1(\bm{f})=\sum_i \Delta f_i$. 

One can show that the Lagrange multiplier $\lambda = - \Delta (\frac{1}{T_i}) = \frac{1}{T_{i+1}}-\frac{1}{T_i}$ by considering a slight modification (non-steady state) of equation (\ref{MaxEntPDF2}) in which in zone $i$ energy $u(\tau)$ is stored for some finite time $\tau$\cite{Dewar:2014ek}. 

Using the equation (\ref{SymmetricIrreversible}) one gets

\begin{equation}
\begin{aligned}
  I(\bm{F})=2 \bm{\lambda}(\bm{F})\bm{F} \propto - \sum_i F_i \Delta\bigg(\frac{1}{T_i}\bigg)= \sum_i \bigg(\frac{\Delta F_i}{T_i}\bigg)
\end{aligned}
\end{equation}

which has the form of entropy production function used by \cite{Paltridge:2007jf}\cite{Lorenz:J80tzZkl}.
  
\section{Summary}

After over 100 years, the arguments of Boltzmann and others on the Loschmidt paradox have become more tangible and very refined through the description sub-macroscopic description provided by the fluctuation theorem.
The research on fluctuation theorems has been immensely productive, providing us with better understanding of thermostatted systems, steady states, dissipation and relaxation processes, keeping the correspondence with linear response theory. Currently, the work in this area is slowly shifting to the desirable quantum realm\cite{Kurchan:2000uh}.

Applications of fluctuation theorem are already giving us knew methods of calculating free energy of large molecules and some early understanding of self replicators and adaptation. The full understanding will almost surely, demand the inclusion of Information theory into the picture. However the current approach of MaxEP, pioneered by Dewar doesn't look as the correct answer as it gives less general results than the fluctuation theorem.  

This work presented some of recent developments in a pedagogical manner, bridging the gaps between the old and the new. For small scales the transition occurred from Onsager and Kubo relations to dissipation theorem and from irreversibility measure protoplast to fluctuation theorems.
On large scales the transition took place from MaxEnt to MaxEP, which although may not hold exactly, certainly revives the unifying spirit of physics of the early days.    
%One gets non-gausian distribution!

\newpage

\bibliographystyle{ieeetr}
\bibliography{Refs}


\end{document}